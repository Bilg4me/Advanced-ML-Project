{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ML Project\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Transformers with Attention Mechanisms**\n",
    "    - **Project**: Develop a specialized Transformer model to capture long-term dependencies and temporal relationships in financial data, based on \"Attention is All You Need\" by Vaswani et al. (2017). We could also use the Temporal Fusion Transformer (TFT) for its efficiency with multi-horizon series, as presented by Lim et al. in \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\" (2021).\n",
    "    - **Objective**: Improve prediction accuracy by using attention mechanisms to capture the influences of distant data points.\n",
    "\n",
    "2. **Variational Autoencoders (VAE) for Feature Engineering and Anomaly Detection**\n",
    "    - **Project**: Use VAEs to compress market data and extract latent representations less sensitive to noise, inspired by \"Auto-Encoding Variational Bayes\" by Kingma and Welling (2013). We will also explore VAE-based approaches for anomaly detection in time series, as detailed in \"Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network\" by Hundman et al. (2018).\n",
    "    - **Objective**: Extract robust latent features and detect anomalies to improve predictions and identify unexpected market variations.\n",
    "\n",
    "3. **Performance Comparison**\n",
    "    - **Objective**: Compare the performance of the two methods (Transformers vs VAE) for time series forecasting.\n",
    "\n",
    "4. **Hyperparameter Fine-Tuning**\n",
    "    - **Objective**: Fine-tune the model hyperparameters to optimize prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xuxu-wei/HybridVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TimeSeriesTransformerForPrediction\n",
    "#from lightning.pytorch import Trainer\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import QuantileLoss, MAE\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../preprocessed_data/training.parquet').dropna()\n",
    "val_df = pd.read_parquet('../preprocessed_data/validation.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"responder_6_lag_1\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples to try the algo\n",
    "df_small = df.iloc[-50000:]\n",
    "val_df_small = val_df.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>...</th>\n",
       "      <th>responder_8</th>\n",
       "      <th>label</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4917242</th>\n",
       "      <td>46545372</td>\n",
       "      <td>1683</td>\n",
       "      <td>540</td>\n",
       "      <td>38</td>\n",
       "      <td>3.273693</td>\n",
       "      <td>3.556780</td>\n",
       "      <td>0.074575</td>\n",
       "      <td>2.914518</td>\n",
       "      <td>2.453409</td>\n",
       "      <td>0.463120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277686</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.028175</td>\n",
       "      <td>-0.080025</td>\n",
       "      <td>2.841541</td>\n",
       "      <td>-0.226218</td>\n",
       "      <td>-0.121201</td>\n",
       "      <td>1.421510</td>\n",
       "      <td>-0.036155</td>\n",
       "      <td>-0.272156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917243</th>\n",
       "      <td>46545373</td>\n",
       "      <td>1683</td>\n",
       "      <td>541</td>\n",
       "      <td>0</td>\n",
       "      <td>2.701212</td>\n",
       "      <td>3.372633</td>\n",
       "      <td>-0.374773</td>\n",
       "      <td>2.995247</td>\n",
       "      <td>3.278934</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046610</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.393132</td>\n",
       "      <td>-0.275410</td>\n",
       "      <td>1.151608</td>\n",
       "      <td>0.406872</td>\n",
       "      <td>0.187856</td>\n",
       "      <td>0.207768</td>\n",
       "      <td>0.045769</td>\n",
       "      <td>0.080846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917244</th>\n",
       "      <td>46545374</td>\n",
       "      <td>1683</td>\n",
       "      <td>541</td>\n",
       "      <td>1</td>\n",
       "      <td>2.063448</td>\n",
       "      <td>3.227684</td>\n",
       "      <td>-0.334587</td>\n",
       "      <td>3.511540</td>\n",
       "      <td>3.150932</td>\n",
       "      <td>0.032244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031631</td>\n",
       "      <td>0</td>\n",
       "      <td>0.800587</td>\n",
       "      <td>0.535868</td>\n",
       "      <td>0.240538</td>\n",
       "      <td>0.232264</td>\n",
       "      <td>0.114425</td>\n",
       "      <td>-0.162787</td>\n",
       "      <td>-0.066659</td>\n",
       "      <td>-0.548071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917245</th>\n",
       "      <td>46545375</td>\n",
       "      <td>1683</td>\n",
       "      <td>541</td>\n",
       "      <td>2</td>\n",
       "      <td>2.165114</td>\n",
       "      <td>3.412851</td>\n",
       "      <td>-0.185964</td>\n",
       "      <td>2.846127</td>\n",
       "      <td>3.366333</td>\n",
       "      <td>-0.106998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255572</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.639018</td>\n",
       "      <td>-0.554432</td>\n",
       "      <td>0.467550</td>\n",
       "      <td>0.073211</td>\n",
       "      <td>0.056951</td>\n",
       "      <td>-0.291103</td>\n",
       "      <td>-0.084555</td>\n",
       "      <td>-0.619661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4917246</th>\n",
       "      <td>46545376</td>\n",
       "      <td>1683</td>\n",
       "      <td>541</td>\n",
       "      <td>3</td>\n",
       "      <td>1.990278</td>\n",
       "      <td>3.259458</td>\n",
       "      <td>-0.162744</td>\n",
       "      <td>3.137269</td>\n",
       "      <td>2.922108</td>\n",
       "      <td>0.421853</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.291902</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.653854</td>\n",
       "      <td>-0.178390</td>\n",
       "      <td>-1.439600</td>\n",
       "      <td>-0.949737</td>\n",
       "      <td>-0.427413</td>\n",
       "      <td>-0.794832</td>\n",
       "      <td>-0.048749</td>\n",
       "      <td>-0.196539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971642</th>\n",
       "      <td>46599772</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>33</td>\n",
       "      <td>1.260113</td>\n",
       "      <td>2.854824</td>\n",
       "      <td>-0.182001</td>\n",
       "      <td>3.030202</td>\n",
       "      <td>2.955101</td>\n",
       "      <td>-0.034863</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.662478</td>\n",
       "      <td>0</td>\n",
       "      <td>1.421281</td>\n",
       "      <td>0.680279</td>\n",
       "      <td>0.839948</td>\n",
       "      <td>0.411006</td>\n",
       "      <td>0.187644</td>\n",
       "      <td>0.273078</td>\n",
       "      <td>-0.016302</td>\n",
       "      <td>-0.182008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971643</th>\n",
       "      <td>46599773</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>34</td>\n",
       "      <td>2.364175</td>\n",
       "      <td>3.328318</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>3.429580</td>\n",
       "      <td>2.893713</td>\n",
       "      <td>0.575843</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.915839</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557703</td>\n",
       "      <td>0.091131</td>\n",
       "      <td>-0.555067</td>\n",
       "      <td>-0.374748</td>\n",
       "      <td>-0.135764</td>\n",
       "      <td>-0.168106</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.005263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971644</th>\n",
       "      <td>46599774</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>35</td>\n",
       "      <td>0.971465</td>\n",
       "      <td>3.334346</td>\n",
       "      <td>-0.092881</td>\n",
       "      <td>3.382863</td>\n",
       "      <td>3.111636</td>\n",
       "      <td>0.644229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359975</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923129</td>\n",
       "      <td>0.792256</td>\n",
       "      <td>-0.687760</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.033282</td>\n",
       "      <td>0.122405</td>\n",
       "      <td>0.026679</td>\n",
       "      <td>-0.025678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971646</th>\n",
       "      <td>46599776</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>37</td>\n",
       "      <td>1.306336</td>\n",
       "      <td>3.129787</td>\n",
       "      <td>-0.023663</td>\n",
       "      <td>3.379802</td>\n",
       "      <td>3.387269</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.646123</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568908</td>\n",
       "      <td>0.304786</td>\n",
       "      <td>-0.029890</td>\n",
       "      <td>1.104309</td>\n",
       "      <td>0.650506</td>\n",
       "      <td>0.459980</td>\n",
       "      <td>-0.007395</td>\n",
       "      <td>-0.160535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971647</th>\n",
       "      <td>46599777</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>38</td>\n",
       "      <td>2.428597</td>\n",
       "      <td>3.204614</td>\n",
       "      <td>0.696010</td>\n",
       "      <td>2.955036</td>\n",
       "      <td>3.043746</td>\n",
       "      <td>0.339810</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196241</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.324545</td>\n",
       "      <td>0.085374</td>\n",
       "      <td>0.258105</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>0.581888</td>\n",
       "      <td>0.065826</td>\n",
       "      <td>0.110606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  date_id  time_id  symbol_id    weight  feature_00  \\\n",
       "4917242  46545372     1683      540         38  3.273693    3.556780   \n",
       "4917243  46545373     1683      541          0  2.701212    3.372633   \n",
       "4917244  46545374     1683      541          1  2.063448    3.227684   \n",
       "4917245  46545375     1683      541          2  2.165114    3.412851   \n",
       "4917246  46545376     1683      541          3  1.990278    3.259458   \n",
       "...           ...      ...      ...        ...       ...         ...   \n",
       "4971642  46599772     1684      967         33  1.260113    2.854824   \n",
       "4971643  46599773     1684      967         34  2.364175    3.328318   \n",
       "4971644  46599774     1684      967         35  0.971465    3.334346   \n",
       "4971646  46599776     1684      967         37  1.306336    3.129787   \n",
       "4971647  46599777     1684      967         38  2.428597    3.204614   \n",
       "\n",
       "         feature_01  feature_02  feature_03  feature_04  ...  responder_8  \\\n",
       "4917242    0.074575    2.914518    2.453409    0.463120  ...     0.277686   \n",
       "4917243   -0.374773    2.995247    3.278934    0.005105  ...    -0.046610   \n",
       "4917244   -0.334587    3.511540    3.150932    0.032244  ...     0.031631   \n",
       "4917245   -0.185964    2.846127    3.366333   -0.106998  ...     0.255572   \n",
       "4917246   -0.162744    3.137269    2.922108    0.421853  ...    -0.291902   \n",
       "...             ...         ...         ...         ...  ...          ...   \n",
       "4971642   -0.182001    3.030202    2.955101   -0.034863  ...    -0.662478   \n",
       "4971643    0.004640    3.429580    2.893713    0.575843  ...    -0.915839   \n",
       "4971644   -0.092881    3.382863    3.111636    0.644229  ...     0.359975   \n",
       "4971646   -0.023663    3.379802    3.387269    0.109540  ...    -0.646123   \n",
       "4971647    0.696010    2.955036    3.043746    0.339810  ...    -0.196241   \n",
       "\n",
       "         label  responder_0_lag_1  responder_1_lag_1  responder_2_lag_1  \\\n",
       "4917242      0          -0.028175          -0.080025           2.841541   \n",
       "4917243      0          -0.393132          -0.275410           1.151608   \n",
       "4917244      0           0.800587           0.535868           0.240538   \n",
       "4917245      2          -1.639018          -0.554432           0.467550   \n",
       "4917246      0          -0.653854          -0.178390          -1.439600   \n",
       "...        ...                ...                ...                ...   \n",
       "4971642      0           1.421281           0.680279           0.839948   \n",
       "4971643      0           0.557703           0.091131          -0.555067   \n",
       "4971644      0           0.923129           0.792256          -0.687760   \n",
       "4971646      0           0.568908           0.304786          -0.029890   \n",
       "4971647      0          -0.324545           0.085374           0.258105   \n",
       "\n",
       "         responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  \\\n",
       "4917242          -0.226218          -0.121201           1.421510   \n",
       "4917243           0.406872           0.187856           0.207768   \n",
       "4917244           0.232264           0.114425          -0.162787   \n",
       "4917245           0.073211           0.056951          -0.291103   \n",
       "4917246          -0.949737          -0.427413          -0.794832   \n",
       "...                    ...                ...                ...   \n",
       "4971642           0.411006           0.187644           0.273078   \n",
       "4971643          -0.374748          -0.135764          -0.168106   \n",
       "4971644           0.040876           0.033282           0.122405   \n",
       "4971646           1.104309           0.650506           0.459980   \n",
       "4971647           0.077070           0.052239           0.581888   \n",
       "\n",
       "         responder_7_lag_1  responder_8_lag_1  \n",
       "4917242          -0.036155          -0.272156  \n",
       "4917243           0.045769           0.080846  \n",
       "4917244          -0.066659          -0.548071  \n",
       "4917245          -0.084555          -0.619661  \n",
       "4917246          -0.048749          -0.196539  \n",
       "...                    ...                ...  \n",
       "4971642          -0.016302          -0.182008  \n",
       "4971643           0.041251           0.005263  \n",
       "4971644           0.026679          -0.025678  \n",
       "4971646          -0.007395          -0.160535  \n",
       "4971647           0.065826           0.110606  \n",
       "\n",
       "[50000 rows x 102 columns]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 50  # lookback window\n",
    "max_prediction_length = 5  # forecast window\n",
    "#training_cutoff = int(len(df) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_82790/3826929958.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small[\"symbol_id\"] = df_small[\"symbol_id\"].astype(\"str\")\n"
     ]
    }
   ],
   "source": [
    "df_small[\"symbol_id\"] = df_small[\"symbol_id\"].astype(\"str\")\n",
    "val_df_small[\"symbol_id\"] = val_df_small[\"symbol_id\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    uint32\n",
       "date_id                int32\n",
       "time_id                int16\n",
       "symbol_id             object\n",
       "weight               float32\n",
       "                      ...   \n",
       "responder_5_lag_1    float32\n",
       "responder_6_lag_1    float32\n",
       "responder_7_lag_1    float32\n",
       "responder_8_lag_1    float32\n",
       "time_idx               int16\n",
       "Length: 104, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.dropna().sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2128308</th>\n",
       "      <td>43756438</td>\n",
       "      <td>1608.0</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>3.937173</td>\n",
       "      <td>2.306764</td>\n",
       "      <td>0.519196</td>\n",
       "      <td>2.637494</td>\n",
       "      <td>2.254404</td>\n",
       "      <td>-0.841473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.162277</td>\n",
       "      <td>-0.116389</td>\n",
       "      <td>1.560023</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>0.343606</td>\n",
       "      <td>0.846384</td>\n",
       "      <td>0.210039</td>\n",
       "      <td>0.088136</td>\n",
       "      <td>0.334838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714380</th>\n",
       "      <td>43342510</td>\n",
       "      <td>1597.0</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475202</td>\n",
       "      <td>-0.058088</td>\n",
       "      <td>-2.612298</td>\n",
       "      <td>-0.046534</td>\n",
       "      <td>0.057987</td>\n",
       "      <td>-1.318175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.592873</td>\n",
       "      <td>1.349861</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>1.579569</td>\n",
       "      <td>0.825219</td>\n",
       "      <td>1.455915</td>\n",
       "      <td>0.738635</td>\n",
       "      <td>0.376521</td>\n",
       "      <td>1.107516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649701</th>\n",
       "      <td>44277831</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>364</td>\n",
       "      <td>5</td>\n",
       "      <td>3.011554</td>\n",
       "      <td>1.774683</td>\n",
       "      <td>-1.365354</td>\n",
       "      <td>1.586449</td>\n",
       "      <td>1.855868</td>\n",
       "      <td>0.517707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.264722</td>\n",
       "      <td>0.778407</td>\n",
       "      <td>-0.526852</td>\n",
       "      <td>0.241921</td>\n",
       "      <td>0.177216</td>\n",
       "      <td>0.372287</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.096059</td>\n",
       "      <td>0.245909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989011</th>\n",
       "      <td>45617141</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>518</td>\n",
       "      <td>9</td>\n",
       "      <td>1.194727</td>\n",
       "      <td>1.728448</td>\n",
       "      <td>0.056828</td>\n",
       "      <td>2.213796</td>\n",
       "      <td>2.091799</td>\n",
       "      <td>-1.545495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.106015</td>\n",
       "      <td>0.136787</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.089944</td>\n",
       "      <td>0.067552</td>\n",
       "      <td>-0.290040</td>\n",
       "      <td>-1.256863</td>\n",
       "      <td>-0.516847</td>\n",
       "      <td>-3.350451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119651</th>\n",
       "      <td>44747781</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>838</td>\n",
       "      <td>17</td>\n",
       "      <td>3.891044</td>\n",
       "      <td>0.739156</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.571764</td>\n",
       "      <td>0.298341</td>\n",
       "      <td>-0.360414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.615973</td>\n",
       "      <td>-0.177908</td>\n",
       "      <td>-0.086522</td>\n",
       "      <td>-0.439819</td>\n",
       "      <td>-0.191102</td>\n",
       "      <td>-0.016968</td>\n",
       "      <td>-0.371733</td>\n",
       "      <td>-0.204741</td>\n",
       "      <td>-0.872424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080413</th>\n",
       "      <td>44708543</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>800</td>\n",
       "      <td>13</td>\n",
       "      <td>2.670314</td>\n",
       "      <td>0.455579</td>\n",
       "      <td>0.923407</td>\n",
       "      <td>0.319387</td>\n",
       "      <td>0.632010</td>\n",
       "      <td>0.082968</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.327784</td>\n",
       "      <td>0.268779</td>\n",
       "      <td>1.165349</td>\n",
       "      <td>0.699005</td>\n",
       "      <td>0.443276</td>\n",
       "      <td>1.196272</td>\n",
       "      <td>0.697565</td>\n",
       "      <td>0.234190</td>\n",
       "      <td>1.262335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963786</th>\n",
       "      <td>42591916</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>884</td>\n",
       "      <td>30</td>\n",
       "      <td>2.354905</td>\n",
       "      <td>2.320300</td>\n",
       "      <td>-1.297566</td>\n",
       "      <td>2.499391</td>\n",
       "      <td>2.726670</td>\n",
       "      <td>1.380740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.309887</td>\n",
       "      <td>0.979838</td>\n",
       "      <td>1.944776</td>\n",
       "      <td>0.420354</td>\n",
       "      <td>0.166707</td>\n",
       "      <td>-0.033193</td>\n",
       "      <td>-0.340363</td>\n",
       "      <td>-0.069098</td>\n",
       "      <td>-0.533852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448471</th>\n",
       "      <td>46076601</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>828</td>\n",
       "      <td>24</td>\n",
       "      <td>1.294774</td>\n",
       "      <td>-0.114172</td>\n",
       "      <td>-1.025281</td>\n",
       "      <td>0.161494</td>\n",
       "      <td>0.267733</td>\n",
       "      <td>-1.050663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376283</td>\n",
       "      <td>-0.302141</td>\n",
       "      <td>-0.045680</td>\n",
       "      <td>0.456144</td>\n",
       "      <td>0.184020</td>\n",
       "      <td>-0.004782</td>\n",
       "      <td>-0.003736</td>\n",
       "      <td>0.015292</td>\n",
       "      <td>-0.024052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291974</th>\n",
       "      <td>43920104</td>\n",
       "      <td>1612.0</td>\n",
       "      <td>706</td>\n",
       "      <td>26</td>\n",
       "      <td>1.639267</td>\n",
       "      <td>2.598802</td>\n",
       "      <td>-1.028428</td>\n",
       "      <td>1.620213</td>\n",
       "      <td>1.803322</td>\n",
       "      <td>-0.750055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>0.269177</td>\n",
       "      <td>0.118191</td>\n",
       "      <td>-0.301266</td>\n",
       "      <td>-0.128761</td>\n",
       "      <td>-0.843805</td>\n",
       "      <td>-0.831479</td>\n",
       "      <td>-0.384089</td>\n",
       "      <td>-1.556916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580327</th>\n",
       "      <td>45208457</td>\n",
       "      <td>1647.0</td>\n",
       "      <td>687</td>\n",
       "      <td>6</td>\n",
       "      <td>1.358670</td>\n",
       "      <td>3.515468</td>\n",
       "      <td>-0.513511</td>\n",
       "      <td>3.021658</td>\n",
       "      <td>2.336688</td>\n",
       "      <td>-0.971272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827921</td>\n",
       "      <td>-1.620434</td>\n",
       "      <td>-0.675703</td>\n",
       "      <td>0.357626</td>\n",
       "      <td>0.175316</td>\n",
       "      <td>0.052195</td>\n",
       "      <td>0.114283</td>\n",
       "      <td>0.088414</td>\n",
       "      <td>0.239030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  date_id  time_id symbol_id    weight  feature_00  \\\n",
       "2128308  43756438   1608.0      364         0  3.937173    2.306764   \n",
       "1714380  43342510   1597.0      109         0  4.475202   -0.058088   \n",
       "2649701  44277831   1622.0      364         5  3.011554    1.774683   \n",
       "3989011  45617141   1658.0      518         9  1.194727    1.728448   \n",
       "3119651  44747781   1634.0      838        17  3.891044    0.739156   \n",
       "...           ...      ...      ...       ...       ...         ...   \n",
       "3080413  44708543   1633.0      800        13  2.670314    0.455579   \n",
       "963786   42591916   1576.0      884        30  2.354905    2.320300   \n",
       "4448471  46076601   1670.0      828        24  1.294774   -0.114172   \n",
       "2291974  43920104   1612.0      706        26  1.639267    2.598802   \n",
       "3580327  45208457   1647.0      687         6  1.358670    3.515468   \n",
       "\n",
       "         feature_01  feature_02  feature_03  feature_04  ...  label  \\\n",
       "2128308    0.519196    2.637494    2.254404   -0.841473  ...    0.0   \n",
       "1714380   -2.612298   -0.046534    0.057987   -1.318175  ...    0.0   \n",
       "2649701   -1.365354    1.586449    1.855868    0.517707  ...    0.0   \n",
       "3989011    0.056828    2.213796    2.091799   -1.545495  ...    0.0   \n",
       "3119651    0.923295    0.571764    0.298341   -0.360414  ...    0.0   \n",
       "...             ...         ...         ...         ...  ...    ...   \n",
       "3080413    0.923407    0.319387    0.632010    0.082968  ...    3.0   \n",
       "963786    -1.297566    2.499391    2.726670    1.380740  ...    0.0   \n",
       "4448471   -1.025281    0.161494    0.267733   -1.050663  ...    0.0   \n",
       "2291974   -1.028428    1.620213    1.803322   -0.750055  ...    0.0   \n",
       "3580327   -0.513511    3.021658    2.336688   -0.971272  ...    0.0   \n",
       "\n",
       "         responder_0_lag_1  responder_1_lag_1  responder_2_lag_1  \\\n",
       "2128308          -0.162277          -0.116389           1.560023   \n",
       "1714380           1.592873           1.349861           0.034853   \n",
       "2649701           1.264722           0.778407          -0.526852   \n",
       "3989011          -0.106015           0.136787           0.622785   \n",
       "3119651          -0.615973          -0.177908          -0.086522   \n",
       "...                    ...                ...                ...   \n",
       "3080413           0.327784           0.268779           1.165349   \n",
       "963786            1.309887           0.979838           1.944776   \n",
       "4448471           0.376283          -0.302141          -0.045680   \n",
       "2291974           0.428165           0.269177           0.118191   \n",
       "3580327           0.827921          -1.620434          -0.675703   \n",
       "\n",
       "         responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  \\\n",
       "2128308           0.979339           0.343606           0.846384   \n",
       "1714380           1.579569           0.825219           1.455915   \n",
       "2649701           0.241921           0.177216           0.372287   \n",
       "3989011           0.089944           0.067552          -0.290040   \n",
       "3119651          -0.439819          -0.191102          -0.016968   \n",
       "...                    ...                ...                ...   \n",
       "3080413           0.699005           0.443276           1.196272   \n",
       "963786            0.420354           0.166707          -0.033193   \n",
       "4448471           0.456144           0.184020          -0.004782   \n",
       "2291974          -0.301266          -0.128761          -0.843805   \n",
       "3580327           0.357626           0.175316           0.052195   \n",
       "\n",
       "         responder_6_lag_1  responder_7_lag_1  responder_8_lag_1  \n",
       "2128308           0.210039           0.088136           0.334838  \n",
       "1714380           0.738635           0.376521           1.107516  \n",
       "2649701           0.144821           0.096059           0.245909  \n",
       "3989011          -1.256863          -0.516847          -3.350451  \n",
       "3119651          -0.371733          -0.204741          -0.872424  \n",
       "...                    ...                ...                ...  \n",
       "3080413           0.697565           0.234190           1.262335  \n",
       "963786           -0.340363          -0.069098          -0.533852  \n",
       "4448471          -0.003736           0.015292          -0.024052  \n",
       "2291974          -0.831479          -0.384089          -1.556916  \n",
       "3580327           0.114283           0.088414           0.239030  \n",
       "\n",
       "[1000000 rows x 103 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(df_small.drop(columns=[\"label\"]), df_small[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_index = [e for e in df.index if e not in df_small.index]\n",
    "df_val = df.loc[val_index].dropna().sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.9071758644357212\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(df_small.drop(columns=[\"label\"]), df_small[\"label\"])\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(df_val.drop(columns=[\"label\"]))\n",
    "rms = mean_squared_error(df_val[\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1493276222104628"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.132206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.255899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.393128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.951314</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.216222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.073650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>-0.789667</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.885828</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-2.371733</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted  true\n",
       "0      -0.132206     0\n",
       "1       0.255899     0\n",
       "2       0.393128     0\n",
       "3      -0.951314    -1\n",
       "4       0.111899     0\n",
       "...          ...   ...\n",
       "99995   1.216222     1\n",
       "99996  -0.073650     0\n",
       "99997  -0.789667    -1\n",
       "99998  -0.885828    -1\n",
       "99999  -2.371733    -3\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predi = pd.DataFrame(y_pred, columns=[\"predicted\"])\n",
    "predi['true'] = df_val[\"label\"].values\n",
    "predi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Temporal Fusion Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of an unique time id column for TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_82790/4116286945.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small['time_idx'] = df_small['date_id'] + df_small['time_id']\n"
     ]
    }
   ],
   "source": [
    "df_small['time_idx'] = df_small['date_id'] + df_small['time_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.657321e+07</td>\n",
       "      <td>1683.684000</td>\n",
       "      <td>592.229720</td>\n",
       "      <td>1.989610</td>\n",
       "      <td>3.227522</td>\n",
       "      <td>-0.109737</td>\n",
       "      <td>3.214889</td>\n",
       "      <td>3.214286</td>\n",
       "      <td>-0.084449</td>\n",
       "      <td>-0.217516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.139762</td>\n",
       "      <td>0.139813</td>\n",
       "      <td>0.148102</td>\n",
       "      <td>0.084448</td>\n",
       "      <td>0.042563</td>\n",
       "      <td>0.089323</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>-0.088894</td>\n",
       "      <td>2275.913720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.592014e+04</td>\n",
       "      <td>0.464918</td>\n",
       "      <td>251.119804</td>\n",
       "      <td>0.909354</td>\n",
       "      <td>0.300719</td>\n",
       "      <td>0.815849</td>\n",
       "      <td>0.300416</td>\n",
       "      <td>0.300179</td>\n",
       "      <td>0.640345</td>\n",
       "      <td>0.709463</td>\n",
       "      <td>...</td>\n",
       "      <td>1.030985</td>\n",
       "      <td>0.901758</td>\n",
       "      <td>0.478356</td>\n",
       "      <td>0.881593</td>\n",
       "      <td>0.556480</td>\n",
       "      <td>0.244032</td>\n",
       "      <td>0.417234</td>\n",
       "      <td>0.101158</td>\n",
       "      <td>0.457286</td>\n",
       "      <td>250.916599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.654537e+07</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.702595</td>\n",
       "      <td>1.952113</td>\n",
       "      <td>-2.421601</td>\n",
       "      <td>1.915817</td>\n",
       "      <td>1.968258</td>\n",
       "      <td>-2.727498</td>\n",
       "      <td>-4.481705</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.000000</td>\n",
       "      <td>-2.663801</td>\n",
       "      <td>-1.111050</td>\n",
       "      <td>-2.362758</td>\n",
       "      <td>-1.229572</td>\n",
       "      <td>-0.503404</td>\n",
       "      <td>-1.009007</td>\n",
       "      <td>-0.243303</td>\n",
       "      <td>-1.199404</td>\n",
       "      <td>1752.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.655855e+07</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>1.217208</td>\n",
       "      <td>3.024758</td>\n",
       "      <td>-0.664834</td>\n",
       "      <td>3.011751</td>\n",
       "      <td>3.012015</td>\n",
       "      <td>-0.544856</td>\n",
       "      <td>-0.524173</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.367741</td>\n",
       "      <td>-0.173446</td>\n",
       "      <td>-0.380127</td>\n",
       "      <td>-0.131301</td>\n",
       "      <td>-0.064531</td>\n",
       "      <td>-0.170024</td>\n",
       "      <td>-0.042360</td>\n",
       "      <td>-0.323712</td>\n",
       "      <td>2080.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.657412e+07</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>634.000000</td>\n",
       "      <td>1.952800</td>\n",
       "      <td>3.227493</td>\n",
       "      <td>-0.133677</td>\n",
       "      <td>3.215489</td>\n",
       "      <td>3.213831</td>\n",
       "      <td>-0.090082</td>\n",
       "      <td>-0.133825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162484</td>\n",
       "      <td>0.035862</td>\n",
       "      <td>-0.028620</td>\n",
       "      <td>0.091651</td>\n",
       "      <td>0.053591</td>\n",
       "      <td>0.122405</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-0.100084</td>\n",
       "      <td>2318.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.658695e+07</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>2.531363</td>\n",
       "      <td>3.429349</td>\n",
       "      <td>0.398711</td>\n",
       "      <td>3.415995</td>\n",
       "      <td>3.416494</td>\n",
       "      <td>0.371238</td>\n",
       "      <td>0.182307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.684143</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.621562</td>\n",
       "      <td>0.350158</td>\n",
       "      <td>0.166294</td>\n",
       "      <td>0.274269</td>\n",
       "      <td>0.072348</td>\n",
       "      <td>0.169723</td>\n",
       "      <td>2484.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.659978e+07</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>967.000000</td>\n",
       "      <td>4.677842</td>\n",
       "      <td>4.442627</td>\n",
       "      <td>3.012100</td>\n",
       "      <td>4.383210</td>\n",
       "      <td>4.357268</td>\n",
       "      <td>2.148427</td>\n",
       "      <td>2.898562</td>\n",
       "      <td>...</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.467610</td>\n",
       "      <td>1.522810</td>\n",
       "      <td>3.424407</td>\n",
       "      <td>2.347394</td>\n",
       "      <td>0.743473</td>\n",
       "      <td>1.421510</td>\n",
       "      <td>0.356702</td>\n",
       "      <td>1.615337</td>\n",
       "      <td>2651.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       date_id       time_id        weight    feature_00  \\\n",
       "count  5.000000e+04  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean   4.657321e+07   1683.684000    592.229720      1.989610      3.227522   \n",
       "std    1.592014e+04      0.464918    251.119804      0.909354      0.300719   \n",
       "min    4.654537e+07   1683.000000     68.000000      0.702595      1.952113   \n",
       "25%    4.655855e+07   1683.000000    396.000000      1.217208      3.024758   \n",
       "50%    4.657412e+07   1684.000000    634.000000      1.952800      3.227493   \n",
       "75%    4.658695e+07   1684.000000    801.000000      2.531363      3.429349   \n",
       "max    4.659978e+07   1684.000000    967.000000      4.677842      4.442627   \n",
       "\n",
       "         feature_01    feature_02    feature_03    feature_04    feature_05  \\\n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean      -0.109737      3.214889      3.214286     -0.084449     -0.217516   \n",
       "std        0.815849      0.300416      0.300179      0.640345      0.709463   \n",
       "min       -2.421601      1.915817      1.968258     -2.727498     -4.481705   \n",
       "25%       -0.664834      3.011751      3.012015     -0.544856     -0.524173   \n",
       "50%       -0.133677      3.215489      3.213831     -0.090082     -0.133825   \n",
       "75%        0.398711      3.415995      3.416494      0.371238      0.182307   \n",
       "max        3.012100      4.383210      4.357268      2.148427      2.898562   \n",
       "\n",
       "       ...         label  responder_0_lag_1  responder_1_lag_1  \\\n",
       "count  ...  50000.000000       50000.000000       50000.000000   \n",
       "mean   ...      0.023460           0.139762           0.139813   \n",
       "std    ...      1.030985           0.901758           0.478356   \n",
       "min    ...    -10.000000          -2.663801          -1.111050   \n",
       "25%    ...      0.000000          -0.367741          -0.173446   \n",
       "50%    ...      0.000000           0.162484           0.035862   \n",
       "75%    ...      0.000000           0.684143           0.421848   \n",
       "max    ...      9.000000           2.467610           1.522810   \n",
       "\n",
       "       responder_2_lag_1  responder_3_lag_1  responder_4_lag_1  \\\n",
       "count       50000.000000       50000.000000       50000.000000   \n",
       "mean            0.148102           0.084448           0.042563   \n",
       "std             0.881593           0.556480           0.244032   \n",
       "min            -2.362758          -1.229572          -0.503404   \n",
       "25%            -0.380127          -0.131301          -0.064531   \n",
       "50%            -0.028620           0.091651           0.053591   \n",
       "75%             0.621562           0.350158           0.166294   \n",
       "max             3.424407           2.347394           0.743473   \n",
       "\n",
       "       responder_5_lag_1  responder_7_lag_1  responder_8_lag_1      time_idx  \n",
       "count       50000.000000       50000.000000       50000.000000  50000.000000  \n",
       "mean            0.089323           0.011148          -0.088894   2275.913720  \n",
       "std             0.417234           0.101158           0.457286    250.916599  \n",
       "min            -1.009007          -0.243303          -1.199404   1752.000000  \n",
       "25%            -0.170024          -0.042360          -0.323712   2080.000000  \n",
       "50%             0.122405           0.004071          -0.100084   2318.000000  \n",
       "75%             0.274269           0.072348           0.169723   2484.000000  \n",
       "max             1.421510           0.356702           1.615337   2651.000000  \n",
       "\n",
       "[8 rows x 102 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"id\",\"date_id\",'time_id']\n",
    "time_varying_unknown_reals = df_small.drop(columns=[\"id\",\"date_id\",'time_id',\"symbol_id\",\"time_idx\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:572: UserWarning: Target scales will be only added for continous targets\n",
      "  warnings.warn(\"Target scales will be only added for continous targets\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    df_small.drop(columns=columns_to_drop),\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"label\",\n",
    "    group_ids=[\"symbol_id\"],\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"symbol_id\"],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    #time_varying_known_reals=[\"time_id\",\"date_id\"],\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    #target_normalizer=GroupNormalizer(transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:143: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=3,  # Quantile outputs\n",
    "    loss=QuantileLoss(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = training.to_dataloader(train=True, batch_size=32, num_workers=0)\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df_small, predict=True, stop_randomization=True)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=32 * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_82790/296535705.py:1: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4158)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    #accelerator=\"cpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:143: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 44.8k\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=8, \n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    #optimizer=\"Ranger\",\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:73: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LearningRateFinder' object has no attribute 'optimal_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[176], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mTuner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_find\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msuggested learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mres\u001b[38;5;241m.\u001b[39msuggestion()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m fig \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mplot(show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, suggest\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/tuner/tuning.py:184\u001b[0m, in \u001b[0;36mTuner.lr_find\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, dataloaders, datamodule, method, min_lr, max_lr, num_training, mode, early_stop_threshold, update_attr, attr_name)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mfit(model, train_dataloaders, val_dataloaders, datamodule)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;241m=\u001b[39m [cb \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mcallbacks \u001b[38;5;28;01mif\u001b[39;00m cb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lr_finder_callback]\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlr_finder_callback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimal_lr\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LearningRateFinder' object has no attribute 'optimal_lr'"
     ]
    }
   ],
   "source": [
    "res = Tuner(trainer).lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 84.2k\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.06760829753919811,\n",
    "    hidden_size=16,\n",
    "    attention_head_size=4,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    #optimizer=\"Ranger\",\n",
    "    reduce_on_plateau_patience=4,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 456    | train\n",
      "3  | prescalers                         | ModuleDict                      | 1.6 K  | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 48     | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 69.3 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 528    | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "84.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "84.2 K    Total params\n",
      "0.337     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index 104 is out of bounds for dimension 0 with size 51",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[179], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:543\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:579\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    573\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    575\u001b[0m     ckpt_path,\n\u001b[1;32m    576\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    577\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    578\u001b[0m )\n\u001b[0;32m--> 579\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:986\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    991\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1027\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1028\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/trainer.py:1057\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1057\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1059\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:135\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/evaluation_loop.py:396\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    390\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    395\u001b[0m )\n\u001b[0;32m--> 396\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/call.py:311\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 311\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    314\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/strategies/strategy.py:411\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/base_model.py:668\u001b[0m, in \u001b[0;36mBaseModel.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    666\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m    667\u001b[0m log, out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(x, y, batch_idx)\n\u001b[0;32m--> 668\u001b[0m log\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidation_step_outputs\u001b[38;5;241m.\u001b[39mappend(log)\n\u001b[1;32m    670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m log\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:520\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.create_log\u001b[0;34m(self, x, y, out, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_log\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y, out, batch_idx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 520\u001b[0m     log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    522\u001b[0m         log[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpretation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_interpretation(out)\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/base_model.py:721\u001b[0m, in \u001b[0;36mBaseModel.create_log\u001b[0;34m(self, x, y, out, batch_idx, prediction_kwargs, quantiles_kwargs)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_metrics(x, y, out, prediction_kwargs\u001b[38;5;241m=\u001b[39mprediction_kwargs)\n\u001b[1;32m    720\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 721\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prediction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantiles_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantiles_kwargs\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/base_model.py:980\u001b[0m, in \u001b[0;36mBaseModel.log_prediction\u001b[0;34m(self, x, out, batch_idx, **kwargs)\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# don't log matplotlib plots if not available\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m log_indices:\n\u001b[0;32m--> 980\u001b[0m     fig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_loss_to_title\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    981\u001b[0m     tag \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_stage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m prediction\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:722\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.plot_prediction\u001b[0;34m(self, x, out, idx, plot_attention, add_loss_to_title, show_future_observed, ax, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m \u001b[38;5;66;03m# add attention on secondary axis\u001b[39;00m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m plot_attention:\n\u001b[0;32m--> 722\u001b[0m     interpretation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpret_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m to_list(fig):\n\u001b[1;32m    724\u001b[0m         ax \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39maxes[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:632\u001b[0m, in \u001b[0;36mTemporalFusionTransformer.interpret_output\u001b[0;34m(self, out, reduction, attention_prediction_horizon)\u001b[0m\n\u001b[1;32m    629\u001b[0m attention[attention \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1e-5\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# histogram of decode and encode lengths\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m encoder_length_histogram \u001b[38;5;241m=\u001b[39m \u001b[43minteger_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoder_lengths\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_encoder_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m decoder_length_histogram \u001b[38;5;241m=\u001b[39m integer_histogram(\n\u001b[1;32m    634\u001b[0m     out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_lengths\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mout[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_variables\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    635\u001b[0m )\n\u001b[1;32m    637\u001b[0m \u001b[38;5;66;03m# mask where decoder and encoder where not applied when averaging variable selection weights\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/utils/_utils.py:38\u001b[0m, in \u001b[0;36minteger_histogram\u001b[0;34m(data, min, max)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mmax\u001b[39m \u001b[38;5;241m=\u001b[39m uniques\u001b[38;5;241m.\u001b[39mmax()\n\u001b[0;32m---> 38\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muniques\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcounts\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hist\n",
      "\u001b[0;31mRuntimeError\u001b[0m: index 104 is out of bounds for dimension 0 with size 51"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
