{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ML Project\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Transformers with Attention Mechanisms**\n",
    "    - **Project**: Develop a specialized Transformer model to capture long-term dependencies and temporal relationships in financial data, based on \"Attention is All You Need\" by Vaswani et al. (2017). We could also use the Temporal Fusion Transformer (TFT) for its efficiency with multi-horizon series, as presented by Lim et al. in \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\" (2021).\n",
    "    - **Objective**: Improve prediction accuracy by using attention mechanisms to capture the influences of distant data points.\n",
    "\n",
    "2. **Variational Autoencoders (VAE) for Feature Engineering and Anomaly Detection**\n",
    "    - **Project**: Use VAEs to compress market data and extract latent representations less sensitive to noise, inspired by \"Auto-Encoding Variational Bayes\" by Kingma and Welling (2013). We will also explore VAE-based approaches for anomaly detection in time series, as detailed in \"Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network\" by Hundman et al. (2018).\n",
    "    - **Objective**: Extract robust latent features and detect anomalies to improve predictions and identify unexpected market variations.\n",
    "\n",
    "3. **Performance Comparison**\n",
    "    - **Objective**: Compare the performance of the two methods (Transformers vs VAE) for time series forecasting.\n",
    "\n",
    "4. **Hyperparameter Fine-Tuning**\n",
    "    - **Objective**: Fine-tune the model hyperparameters to optimize prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xuxu-wei/HybridVAE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
