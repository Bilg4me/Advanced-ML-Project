{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced ML Project\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "1. **Transformers with Attention Mechanisms**\n",
    "    - **Project**: Develop a specialized Transformer model to capture long-term dependencies and temporal relationships in financial data, based on \"Attention is All You Need\" by Vaswani et al. (2017). We could also use the Temporal Fusion Transformer (TFT) for its efficiency with multi-horizon series, as presented by Lim et al. in \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\" (2021).\n",
    "    - **Objective**: Improve prediction accuracy by using attention mechanisms to capture the influences of distant data points.\n",
    "\n",
    "2. **Variational Autoencoders (VAE) for Feature Engineering and Anomaly Detection**\n",
    "    - **Project**: Use VAEs to compress market data and extract latent representations less sensitive to noise, inspired by \"Auto-Encoding Variational Bayes\" by Kingma and Welling (2013). We will also explore VAE-based approaches for anomaly detection in time series, as detailed in \"Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network\" by Hundman et al. (2018).\n",
    "    - **Objective**: Extract robust latent features and detect anomalies to improve predictions and identify unexpected market variations.\n",
    "\n",
    "3. **Performance Comparison**\n",
    "    - **Objective**: Compare the performance of the two methods (Transformers vs VAE) for time series forecasting.\n",
    "\n",
    "4. **Hyperparameter Fine-Tuning**\n",
    "    - **Objective**: Fine-tune the model hyperparameters to optimize prediction performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/xuxu-wei/HybridVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TimeSeriesTransformerForPrediction\n",
    "#from lightning.pytorch import Trainer\n",
    "from pytorch_forecasting import TimeSeriesDataSet, TemporalFusionTransformer, Baseline\n",
    "from pytorch_forecasting.metrics import QuantileLoss, MAE, RMSE\n",
    "from pytorch_forecasting.data import GroupNormalizer\n",
    "from pytorch_forecasting.data.encoders import NaNLabelEncoder\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('../preprocessed_data/training.parquet').dropna()\n",
    "test_df = pd.read_parquet('../preprocessed_data/test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=\"responder_6_lag_1\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples to try the algo\n",
    "df_small = df.iloc[-500000:]\n",
    "test_df_small = test_df.sample(100000).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>335594</th>\n",
       "      <td>46935372</td>\n",
       "      <td>1693</td>\n",
       "      <td>860</td>\n",
       "      <td>38</td>\n",
       "      <td>2.854656</td>\n",
       "      <td>3.214393</td>\n",
       "      <td>-0.584847</td>\n",
       "      <td>3.289541</td>\n",
       "      <td>2.728956</td>\n",
       "      <td>-0.201064</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.368417</td>\n",
       "      <td>-0.532117</td>\n",
       "      <td>-1.799991</td>\n",
       "      <td>0.084960</td>\n",
       "      <td>0.026816</td>\n",
       "      <td>-0.455079</td>\n",
       "      <td>0.327292</td>\n",
       "      <td>0.263015</td>\n",
       "      <td>0.497859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136187</th>\n",
       "      <td>46735965</td>\n",
       "      <td>1688</td>\n",
       "      <td>587</td>\n",
       "      <td>38</td>\n",
       "      <td>3.278609</td>\n",
       "      <td>3.384902</td>\n",
       "      <td>-0.193709</td>\n",
       "      <td>3.874621</td>\n",
       "      <td>3.343481</td>\n",
       "      <td>-0.075283</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.262724</td>\n",
       "      <td>0.072253</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>-0.035241</td>\n",
       "      <td>-0.033427</td>\n",
       "      <td>0.103239</td>\n",
       "      <td>-0.022854</td>\n",
       "      <td>0.015629</td>\n",
       "      <td>-0.085243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123345</th>\n",
       "      <td>46723123</td>\n",
       "      <td>1688</td>\n",
       "      <td>258</td>\n",
       "      <td>27</td>\n",
       "      <td>1.978096</td>\n",
       "      <td>3.390928</td>\n",
       "      <td>-0.108632</td>\n",
       "      <td>3.360468</td>\n",
       "      <td>3.256377</td>\n",
       "      <td>-0.124128</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.544125</td>\n",
       "      <td>-0.071468</td>\n",
       "      <td>-0.318722</td>\n",
       "      <td>-0.223752</td>\n",
       "      <td>-0.160193</td>\n",
       "      <td>0.298253</td>\n",
       "      <td>0.204016</td>\n",
       "      <td>0.126124</td>\n",
       "      <td>0.410063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444809</th>\n",
       "      <td>47044587</td>\n",
       "      <td>1696</td>\n",
       "      <td>777</td>\n",
       "      <td>11</td>\n",
       "      <td>1.748166</td>\n",
       "      <td>2.368846</td>\n",
       "      <td>-0.710601</td>\n",
       "      <td>2.734649</td>\n",
       "      <td>2.126140</td>\n",
       "      <td>-0.268404</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.532444</td>\n",
       "      <td>0.098892</td>\n",
       "      <td>0.966809</td>\n",
       "      <td>0.417199</td>\n",
       "      <td>0.241041</td>\n",
       "      <td>0.522957</td>\n",
       "      <td>0.008331</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.004970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262079</th>\n",
       "      <td>46861857</td>\n",
       "      <td>1691</td>\n",
       "      <td>911</td>\n",
       "      <td>38</td>\n",
       "      <td>3.028034</td>\n",
       "      <td>2.812776</td>\n",
       "      <td>-0.039425</td>\n",
       "      <td>2.994819</td>\n",
       "      <td>2.936656</td>\n",
       "      <td>-0.132411</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.050007</td>\n",
       "      <td>0.279277</td>\n",
       "      <td>1.290365</td>\n",
       "      <td>0.038503</td>\n",
       "      <td>0.023583</td>\n",
       "      <td>0.592896</td>\n",
       "      <td>-0.170020</td>\n",
       "      <td>-0.023192</td>\n",
       "      <td>-0.300870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246708</th>\n",
       "      <td>46846486</td>\n",
       "      <td>1691</td>\n",
       "      <td>517</td>\n",
       "      <td>33</td>\n",
       "      <td>1.197254</td>\n",
       "      <td>2.934187</td>\n",
       "      <td>-0.196961</td>\n",
       "      <td>3.017119</td>\n",
       "      <td>3.089595</td>\n",
       "      <td>-0.344513</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.225500</td>\n",
       "      <td>0.581439</td>\n",
       "      <td>0.853549</td>\n",
       "      <td>-0.206464</td>\n",
       "      <td>-0.084373</td>\n",
       "      <td>0.237839</td>\n",
       "      <td>-0.053865</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>-0.095834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319426</th>\n",
       "      <td>46919204</td>\n",
       "      <td>1693</td>\n",
       "      <td>446</td>\n",
       "      <td>16</td>\n",
       "      <td>4.651937</td>\n",
       "      <td>2.704496</td>\n",
       "      <td>-1.714473</td>\n",
       "      <td>3.368670</td>\n",
       "      <td>3.725866</td>\n",
       "      <td>0.712718</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.200018</td>\n",
       "      <td>0.417554</td>\n",
       "      <td>-0.097555</td>\n",
       "      <td>0.414277</td>\n",
       "      <td>0.243810</td>\n",
       "      <td>0.231836</td>\n",
       "      <td>0.116222</td>\n",
       "      <td>0.112058</td>\n",
       "      <td>0.265302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467847</th>\n",
       "      <td>47067625</td>\n",
       "      <td>1697</td>\n",
       "      <td>404</td>\n",
       "      <td>35</td>\n",
       "      <td>0.956365</td>\n",
       "      <td>2.283567</td>\n",
       "      <td>-0.378630</td>\n",
       "      <td>1.816175</td>\n",
       "      <td>2.158223</td>\n",
       "      <td>-0.481388</td>\n",
       "      <td>...</td>\n",
       "      <td>-2</td>\n",
       "      <td>1.407528</td>\n",
       "      <td>0.396527</td>\n",
       "      <td>-0.489022</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-2.126582</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.058946</td>\n",
       "      <td>0.071137</td>\n",
       "      <td>0.116078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480905</th>\n",
       "      <td>47080683</td>\n",
       "      <td>1697</td>\n",
       "      <td>739</td>\n",
       "      <td>28</td>\n",
       "      <td>1.377110</td>\n",
       "      <td>2.092795</td>\n",
       "      <td>0.326302</td>\n",
       "      <td>1.834910</td>\n",
       "      <td>1.995446</td>\n",
       "      <td>-0.605290</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.821258</td>\n",
       "      <td>-1.112829</td>\n",
       "      <td>0.138508</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-4.947656</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.176841</td>\n",
       "      <td>0.110973</td>\n",
       "      <td>0.401314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224428</th>\n",
       "      <td>46824206</td>\n",
       "      <td>1690</td>\n",
       "      <td>914</td>\n",
       "      <td>22</td>\n",
       "      <td>1.437689</td>\n",
       "      <td>2.999625</td>\n",
       "      <td>0.075657</td>\n",
       "      <td>2.707470</td>\n",
       "      <td>2.989875</td>\n",
       "      <td>0.619751</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.726913</td>\n",
       "      <td>0.159932</td>\n",
       "      <td>0.359914</td>\n",
       "      <td>-0.199885</td>\n",
       "      <td>-0.117921</td>\n",
       "      <td>0.731614</td>\n",
       "      <td>-0.137172</td>\n",
       "      <td>-0.041723</td>\n",
       "      <td>-0.287411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90390 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id  date_id  time_id  symbol_id    weight  feature_00  \\\n",
       "335594  46935372     1693      860         38  2.854656    3.214393   \n",
       "136187  46735965     1688      587         38  3.278609    3.384902   \n",
       "123345  46723123     1688      258         27  1.978096    3.390928   \n",
       "444809  47044587     1696      777         11  1.748166    2.368846   \n",
       "262079  46861857     1691      911         38  3.028034    2.812776   \n",
       "...          ...      ...      ...        ...       ...         ...   \n",
       "246708  46846486     1691      517         33  1.197254    2.934187   \n",
       "319426  46919204     1693      446         16  4.651937    2.704496   \n",
       "467847  47067625     1697      404         35  0.956365    2.283567   \n",
       "480905  47080683     1697      739         28  1.377110    2.092795   \n",
       "224428  46824206     1690      914         22  1.437689    2.999625   \n",
       "\n",
       "        feature_01  feature_02  feature_03  feature_04  ...  label  \\\n",
       "335594   -0.584847    3.289541    2.728956   -0.201064  ...      0   \n",
       "136187   -0.193709    3.874621    3.343481   -0.075283  ...      0   \n",
       "123345   -0.108632    3.360468    3.256377   -0.124128  ...      0   \n",
       "444809   -0.710601    2.734649    2.126140   -0.268404  ...      0   \n",
       "262079   -0.039425    2.994819    2.936656   -0.132411  ...      0   \n",
       "...            ...         ...         ...         ...  ...    ...   \n",
       "246708   -0.196961    3.017119    3.089595   -0.344513  ...      0   \n",
       "319426   -1.714473    3.368670    3.725866    0.712718  ...      0   \n",
       "467847   -0.378630    1.816175    2.158223   -0.481388  ...     -2   \n",
       "480905    0.326302    1.834910    1.995446   -0.605290  ...      0   \n",
       "224428    0.075657    2.707470    2.989875    0.619751  ...      0   \n",
       "\n",
       "        responder_0_lag_1  responder_1_lag_1  responder_2_lag_1  \\\n",
       "335594          -0.368417          -0.532117          -1.799991   \n",
       "136187          -0.262724           0.072253           0.119085   \n",
       "123345          -0.544125          -0.071468          -0.318722   \n",
       "444809          -0.532444           0.098892           0.966809   \n",
       "262079          -0.050007           0.279277           1.290365   \n",
       "...                   ...                ...                ...   \n",
       "246708           1.225500           0.581439           0.853549   \n",
       "319426          -0.200018           0.417554          -0.097555   \n",
       "467847           1.407528           0.396527          -0.489022   \n",
       "480905          -0.821258          -1.112829           0.138508   \n",
       "224428          -0.726913           0.159932           0.359914   \n",
       "\n",
       "        responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  \\\n",
       "335594           0.084960           0.026816          -0.455079   \n",
       "136187          -0.035241          -0.033427           0.103239   \n",
       "123345          -0.223752          -0.160193           0.298253   \n",
       "444809           0.417199           0.241041           0.522957   \n",
       "262079           0.038503           0.023583           0.592896   \n",
       "...                   ...                ...                ...   \n",
       "246708          -0.206464          -0.084373           0.237839   \n",
       "319426           0.414277           0.243810           0.231836   \n",
       "467847          -5.000000          -2.126582          -5.000000   \n",
       "480905          -5.000000          -4.947656          -5.000000   \n",
       "224428          -0.199885          -0.117921           0.731614   \n",
       "\n",
       "        responder_6_lag_1  responder_7_lag_1  responder_8_lag_1  \n",
       "335594           0.327292           0.263015           0.497859  \n",
       "136187          -0.022854           0.015629          -0.085243  \n",
       "123345           0.204016           0.126124           0.410063  \n",
       "444809           0.008331           0.035156           0.004970  \n",
       "262079          -0.170020          -0.023192          -0.300870  \n",
       "...                   ...                ...                ...  \n",
       "246708          -0.053865           0.001165          -0.095834  \n",
       "319426           0.116222           0.112058           0.265302  \n",
       "467847           0.058946           0.071137           0.116078  \n",
       "480905           0.176841           0.110973           0.401314  \n",
       "224428          -0.137172          -0.041723          -0.287411  \n",
       "\n",
       "[90390 rows x 103 columns]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_length = 50  # lookback window\n",
    "max_prediction_length = 10  # forecast window\n",
    "#training_cutoff = int(len(df) * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_86549/266678574.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small[\"symbol_id\"] = df_small[\"symbol_id\"].astype(\"str\")\n"
     ]
    }
   ],
   "source": [
    "df_small[\"symbol_id\"] = df_small[\"symbol_id\"].astype(\"str\")\n",
    "test_df_small[\"symbol_id\"] = test_df_small[\"symbol_id\"].astype(\"str\")\n",
    "#val_df_small[\"symbol_id\"] = val_df_small[\"symbol_id\"].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                    uint32\n",
       "date_id                int32\n",
       "time_id                int16\n",
       "symbol_id             object\n",
       "weight               float32\n",
       "                      ...   \n",
       "responder_5_lag_1    float32\n",
       "responder_6_lag_1    float32\n",
       "responder_7_lag_1    float32\n",
       "responder_8_lag_1    float32\n",
       "time_idx               int16\n",
       "Length: 104, dtype: object"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_small = df.dropna().sample(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_6_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2128308</th>\n",
       "      <td>43756438</td>\n",
       "      <td>1608.0</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>3.937173</td>\n",
       "      <td>2.306764</td>\n",
       "      <td>0.519196</td>\n",
       "      <td>2.637494</td>\n",
       "      <td>2.254404</td>\n",
       "      <td>-0.841473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.162277</td>\n",
       "      <td>-0.116389</td>\n",
       "      <td>1.560023</td>\n",
       "      <td>0.979339</td>\n",
       "      <td>0.343606</td>\n",
       "      <td>0.846384</td>\n",
       "      <td>0.210039</td>\n",
       "      <td>0.088136</td>\n",
       "      <td>0.334838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1714380</th>\n",
       "      <td>43342510</td>\n",
       "      <td>1597.0</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475202</td>\n",
       "      <td>-0.058088</td>\n",
       "      <td>-2.612298</td>\n",
       "      <td>-0.046534</td>\n",
       "      <td>0.057987</td>\n",
       "      <td>-1.318175</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.592873</td>\n",
       "      <td>1.349861</td>\n",
       "      <td>0.034853</td>\n",
       "      <td>1.579569</td>\n",
       "      <td>0.825219</td>\n",
       "      <td>1.455915</td>\n",
       "      <td>0.738635</td>\n",
       "      <td>0.376521</td>\n",
       "      <td>1.107516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2649701</th>\n",
       "      <td>44277831</td>\n",
       "      <td>1622.0</td>\n",
       "      <td>364</td>\n",
       "      <td>5</td>\n",
       "      <td>3.011554</td>\n",
       "      <td>1.774683</td>\n",
       "      <td>-1.365354</td>\n",
       "      <td>1.586449</td>\n",
       "      <td>1.855868</td>\n",
       "      <td>0.517707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.264722</td>\n",
       "      <td>0.778407</td>\n",
       "      <td>-0.526852</td>\n",
       "      <td>0.241921</td>\n",
       "      <td>0.177216</td>\n",
       "      <td>0.372287</td>\n",
       "      <td>0.144821</td>\n",
       "      <td>0.096059</td>\n",
       "      <td>0.245909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3989011</th>\n",
       "      <td>45617141</td>\n",
       "      <td>1658.0</td>\n",
       "      <td>518</td>\n",
       "      <td>9</td>\n",
       "      <td>1.194727</td>\n",
       "      <td>1.728448</td>\n",
       "      <td>0.056828</td>\n",
       "      <td>2.213796</td>\n",
       "      <td>2.091799</td>\n",
       "      <td>-1.545495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.106015</td>\n",
       "      <td>0.136787</td>\n",
       "      <td>0.622785</td>\n",
       "      <td>0.089944</td>\n",
       "      <td>0.067552</td>\n",
       "      <td>-0.290040</td>\n",
       "      <td>-1.256863</td>\n",
       "      <td>-0.516847</td>\n",
       "      <td>-3.350451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3119651</th>\n",
       "      <td>44747781</td>\n",
       "      <td>1634.0</td>\n",
       "      <td>838</td>\n",
       "      <td>17</td>\n",
       "      <td>3.891044</td>\n",
       "      <td>0.739156</td>\n",
       "      <td>0.923295</td>\n",
       "      <td>0.571764</td>\n",
       "      <td>0.298341</td>\n",
       "      <td>-0.360414</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.615973</td>\n",
       "      <td>-0.177908</td>\n",
       "      <td>-0.086522</td>\n",
       "      <td>-0.439819</td>\n",
       "      <td>-0.191102</td>\n",
       "      <td>-0.016968</td>\n",
       "      <td>-0.371733</td>\n",
       "      <td>-0.204741</td>\n",
       "      <td>-0.872424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3080413</th>\n",
       "      <td>44708543</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>800</td>\n",
       "      <td>13</td>\n",
       "      <td>2.670314</td>\n",
       "      <td>0.455579</td>\n",
       "      <td>0.923407</td>\n",
       "      <td>0.319387</td>\n",
       "      <td>0.632010</td>\n",
       "      <td>0.082968</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.327784</td>\n",
       "      <td>0.268779</td>\n",
       "      <td>1.165349</td>\n",
       "      <td>0.699005</td>\n",
       "      <td>0.443276</td>\n",
       "      <td>1.196272</td>\n",
       "      <td>0.697565</td>\n",
       "      <td>0.234190</td>\n",
       "      <td>1.262335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963786</th>\n",
       "      <td>42591916</td>\n",
       "      <td>1576.0</td>\n",
       "      <td>884</td>\n",
       "      <td>30</td>\n",
       "      <td>2.354905</td>\n",
       "      <td>2.320300</td>\n",
       "      <td>-1.297566</td>\n",
       "      <td>2.499391</td>\n",
       "      <td>2.726670</td>\n",
       "      <td>1.380740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.309887</td>\n",
       "      <td>0.979838</td>\n",
       "      <td>1.944776</td>\n",
       "      <td>0.420354</td>\n",
       "      <td>0.166707</td>\n",
       "      <td>-0.033193</td>\n",
       "      <td>-0.340363</td>\n",
       "      <td>-0.069098</td>\n",
       "      <td>-0.533852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4448471</th>\n",
       "      <td>46076601</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>828</td>\n",
       "      <td>24</td>\n",
       "      <td>1.294774</td>\n",
       "      <td>-0.114172</td>\n",
       "      <td>-1.025281</td>\n",
       "      <td>0.161494</td>\n",
       "      <td>0.267733</td>\n",
       "      <td>-1.050663</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.376283</td>\n",
       "      <td>-0.302141</td>\n",
       "      <td>-0.045680</td>\n",
       "      <td>0.456144</td>\n",
       "      <td>0.184020</td>\n",
       "      <td>-0.004782</td>\n",
       "      <td>-0.003736</td>\n",
       "      <td>0.015292</td>\n",
       "      <td>-0.024052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2291974</th>\n",
       "      <td>43920104</td>\n",
       "      <td>1612.0</td>\n",
       "      <td>706</td>\n",
       "      <td>26</td>\n",
       "      <td>1.639267</td>\n",
       "      <td>2.598802</td>\n",
       "      <td>-1.028428</td>\n",
       "      <td>1.620213</td>\n",
       "      <td>1.803322</td>\n",
       "      <td>-0.750055</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.428165</td>\n",
       "      <td>0.269177</td>\n",
       "      <td>0.118191</td>\n",
       "      <td>-0.301266</td>\n",
       "      <td>-0.128761</td>\n",
       "      <td>-0.843805</td>\n",
       "      <td>-0.831479</td>\n",
       "      <td>-0.384089</td>\n",
       "      <td>-1.556916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3580327</th>\n",
       "      <td>45208457</td>\n",
       "      <td>1647.0</td>\n",
       "      <td>687</td>\n",
       "      <td>6</td>\n",
       "      <td>1.358670</td>\n",
       "      <td>3.515468</td>\n",
       "      <td>-0.513511</td>\n",
       "      <td>3.021658</td>\n",
       "      <td>2.336688</td>\n",
       "      <td>-0.971272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.827921</td>\n",
       "      <td>-1.620434</td>\n",
       "      <td>-0.675703</td>\n",
       "      <td>0.357626</td>\n",
       "      <td>0.175316</td>\n",
       "      <td>0.052195</td>\n",
       "      <td>0.114283</td>\n",
       "      <td>0.088414</td>\n",
       "      <td>0.239030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  date_id  time_id symbol_id    weight  feature_00  \\\n",
       "2128308  43756438   1608.0      364         0  3.937173    2.306764   \n",
       "1714380  43342510   1597.0      109         0  4.475202   -0.058088   \n",
       "2649701  44277831   1622.0      364         5  3.011554    1.774683   \n",
       "3989011  45617141   1658.0      518         9  1.194727    1.728448   \n",
       "3119651  44747781   1634.0      838        17  3.891044    0.739156   \n",
       "...           ...      ...      ...       ...       ...         ...   \n",
       "3080413  44708543   1633.0      800        13  2.670314    0.455579   \n",
       "963786   42591916   1576.0      884        30  2.354905    2.320300   \n",
       "4448471  46076601   1670.0      828        24  1.294774   -0.114172   \n",
       "2291974  43920104   1612.0      706        26  1.639267    2.598802   \n",
       "3580327  45208457   1647.0      687         6  1.358670    3.515468   \n",
       "\n",
       "         feature_01  feature_02  feature_03  feature_04  ...  label  \\\n",
       "2128308    0.519196    2.637494    2.254404   -0.841473  ...    0.0   \n",
       "1714380   -2.612298   -0.046534    0.057987   -1.318175  ...    0.0   \n",
       "2649701   -1.365354    1.586449    1.855868    0.517707  ...    0.0   \n",
       "3989011    0.056828    2.213796    2.091799   -1.545495  ...    0.0   \n",
       "3119651    0.923295    0.571764    0.298341   -0.360414  ...    0.0   \n",
       "...             ...         ...         ...         ...  ...    ...   \n",
       "3080413    0.923407    0.319387    0.632010    0.082968  ...    3.0   \n",
       "963786    -1.297566    2.499391    2.726670    1.380740  ...    0.0   \n",
       "4448471   -1.025281    0.161494    0.267733   -1.050663  ...    0.0   \n",
       "2291974   -1.028428    1.620213    1.803322   -0.750055  ...    0.0   \n",
       "3580327   -0.513511    3.021658    2.336688   -0.971272  ...    0.0   \n",
       "\n",
       "         responder_0_lag_1  responder_1_lag_1  responder_2_lag_1  \\\n",
       "2128308          -0.162277          -0.116389           1.560023   \n",
       "1714380           1.592873           1.349861           0.034853   \n",
       "2649701           1.264722           0.778407          -0.526852   \n",
       "3989011          -0.106015           0.136787           0.622785   \n",
       "3119651          -0.615973          -0.177908          -0.086522   \n",
       "...                    ...                ...                ...   \n",
       "3080413           0.327784           0.268779           1.165349   \n",
       "963786            1.309887           0.979838           1.944776   \n",
       "4448471           0.376283          -0.302141          -0.045680   \n",
       "2291974           0.428165           0.269177           0.118191   \n",
       "3580327           0.827921          -1.620434          -0.675703   \n",
       "\n",
       "         responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  \\\n",
       "2128308           0.979339           0.343606           0.846384   \n",
       "1714380           1.579569           0.825219           1.455915   \n",
       "2649701           0.241921           0.177216           0.372287   \n",
       "3989011           0.089944           0.067552          -0.290040   \n",
       "3119651          -0.439819          -0.191102          -0.016968   \n",
       "...                    ...                ...                ...   \n",
       "3080413           0.699005           0.443276           1.196272   \n",
       "963786            0.420354           0.166707          -0.033193   \n",
       "4448471           0.456144           0.184020          -0.004782   \n",
       "2291974          -0.301266          -0.128761          -0.843805   \n",
       "3580327           0.357626           0.175316           0.052195   \n",
       "\n",
       "         responder_6_lag_1  responder_7_lag_1  responder_8_lag_1  \n",
       "2128308           0.210039           0.088136           0.334838  \n",
       "1714380           0.738635           0.376521           1.107516  \n",
       "2649701           0.144821           0.096059           0.245909  \n",
       "3989011          -1.256863          -0.516847          -3.350451  \n",
       "3119651          -0.371733          -0.204741          -0.872424  \n",
       "...                    ...                ...                ...  \n",
       "3080413           0.697565           0.234190           1.262335  \n",
       "963786           -0.340363          -0.069098          -0.533852  \n",
       "4448471          -0.003736           0.015292          -0.024052  \n",
       "2291974          -0.831479          -0.384089          -1.556916  \n",
       "3580327           0.114283           0.088414           0.239030  \n",
       "\n",
       "[1000000 rows x 103 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(df_small.drop(columns=[\"label\"]), df_small[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_index = [e for e in df.index if e not in df_small.index]\n",
    "df_val = df.loc[val_index].dropna().sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficient of determination: 0.9071758644357212\n"
     ]
    }
   ],
   "source": [
    "r_sq = model.score(df_small.drop(columns=[\"label\"]), df_small[\"label\"])\n",
    "print('coefficient of determination:', r_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(df_val.drop(columns=[\"label\"]))\n",
    "rms = mean_squared_error(df_val[\"label\"], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1493276222104628"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.132206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.255899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.393128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.951314</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1.216222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>-0.073650</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>-0.789667</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>-0.885828</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>-2.371733</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       predicted  true\n",
       "0      -0.132206     0\n",
       "1       0.255899     0\n",
       "2       0.393128     0\n",
       "3      -0.951314    -1\n",
       "4       0.111899     0\n",
       "...          ...   ...\n",
       "99995   1.216222     1\n",
       "99996  -0.073650     0\n",
       "99997  -0.789667    -1\n",
       "99998  -0.885828    -1\n",
       "99999  -2.371733    -3\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predi = pd.DataFrame(y_pred, columns=[\"predicted\"])\n",
    "predi['true'] = df_val[\"label\"].values\n",
    "predi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Temporal Fusion Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of an unique time id column for TimeSeriesDataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.DataFrame(pd.concat([df,test_df]).groupby(['date_id','time_id']).count().index,columns=[\"date_time\"]).reset_index().rename(columns={\"index\":\"time_idx\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_idx</th>\n",
       "      <th>date_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(1552, 68)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>(1552, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>(1552, 70)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>(1552, 71)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>(1552, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132347</th>\n",
       "      <td>132347</td>\n",
       "      <td>(1698, 963)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132348</th>\n",
       "      <td>132348</td>\n",
       "      <td>(1698, 964)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132349</th>\n",
       "      <td>132349</td>\n",
       "      <td>(1698, 965)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132350</th>\n",
       "      <td>132350</td>\n",
       "      <td>(1698, 966)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132351</th>\n",
       "      <td>132351</td>\n",
       "      <td>(1698, 967)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132352 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        time_idx    date_time\n",
       "0              0   (1552, 68)\n",
       "1              1   (1552, 69)\n",
       "2              2   (1552, 70)\n",
       "3              3   (1552, 71)\n",
       "4              4   (1552, 72)\n",
       "...          ...          ...\n",
       "132347    132347  (1698, 963)\n",
       "132348    132348  (1698, 964)\n",
       "132349    132349  (1698, 965)\n",
       "132350    132350  (1698, 966)\n",
       "132351    132351  (1698, 967)\n",
       "\n",
       "[132352 rows x 2 columns]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_86549/2417218599.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small[\"date_time\"] = list(zip(df_small[\"date_id\"], df_small[\"time_id\"]))\n",
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_86549/2417218599.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small[\"time_idx\"] = df_small[\"date_time\"].map(dict(zip(mapping[\"date_time\"], mapping[\"time_idx\"])))\n"
     ]
    }
   ],
   "source": [
    "df_small[\"date_time\"] = list(zip(df_small[\"date_id\"], df_small[\"time_id\"]))\n",
    "df_small[\"time_idx\"] = df_small[\"date_time\"].map(dict(zip(mapping[\"date_time\"], mapping[\"time_idx\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_small[\"date_time\"] = list(zip(test_df_small[\"date_id\"], test_df_small[\"time_id\"]))\n",
    "test_df_small[\"time_idx\"] = test_df_small[\"date_time\"].map(dict(zip(mapping[\"date_time\"], mapping[\"time_idx\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_val = df_small[\"time_idx\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_86549/1245178943.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small['time_idx'] = df_small['time_idx']-first_val\n"
     ]
    }
   ],
   "source": [
    "df_small['time_idx'] = df_small['time_idx']-first_val\n",
    "test_df_small['time_idx'] = test_df_small['time_idx']-first_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>...</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>time_idx_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.657321e+07</td>\n",
       "      <td>1683.684000</td>\n",
       "      <td>592.229720</td>\n",
       "      <td>1.989610</td>\n",
       "      <td>3.227522</td>\n",
       "      <td>-0.109737</td>\n",
       "      <td>3.214889</td>\n",
       "      <td>3.214286</td>\n",
       "      <td>-0.084449</td>\n",
       "      <td>-0.217516</td>\n",
       "      <td>...</td>\n",
       "      <td>0.139762</td>\n",
       "      <td>0.139813</td>\n",
       "      <td>0.148102</td>\n",
       "      <td>0.084448</td>\n",
       "      <td>0.042563</td>\n",
       "      <td>0.089323</td>\n",
       "      <td>0.011148</td>\n",
       "      <td>-0.088894</td>\n",
       "      <td>2275.913720</td>\n",
       "      <td>659.17028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.592014e+04</td>\n",
       "      <td>0.464918</td>\n",
       "      <td>251.119804</td>\n",
       "      <td>0.909354</td>\n",
       "      <td>0.300719</td>\n",
       "      <td>0.815849</td>\n",
       "      <td>0.300416</td>\n",
       "      <td>0.300179</td>\n",
       "      <td>0.640345</td>\n",
       "      <td>0.709463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.901758</td>\n",
       "      <td>0.478356</td>\n",
       "      <td>0.881593</td>\n",
       "      <td>0.556480</td>\n",
       "      <td>0.244032</td>\n",
       "      <td>0.417234</td>\n",
       "      <td>0.101158</td>\n",
       "      <td>0.457286</td>\n",
       "      <td>250.916599</td>\n",
       "      <td>382.27245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.654537e+07</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.702595</td>\n",
       "      <td>1.952113</td>\n",
       "      <td>-2.421601</td>\n",
       "      <td>1.915817</td>\n",
       "      <td>1.968258</td>\n",
       "      <td>-2.727498</td>\n",
       "      <td>-4.481705</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.663801</td>\n",
       "      <td>-1.111050</td>\n",
       "      <td>-2.362758</td>\n",
       "      <td>-1.229572</td>\n",
       "      <td>-0.503404</td>\n",
       "      <td>-1.009007</td>\n",
       "      <td>-0.243303</td>\n",
       "      <td>-1.199404</td>\n",
       "      <td>1752.000000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.655855e+07</td>\n",
       "      <td>1683.000000</td>\n",
       "      <td>396.000000</td>\n",
       "      <td>1.217208</td>\n",
       "      <td>3.024758</td>\n",
       "      <td>-0.664834</td>\n",
       "      <td>3.011751</td>\n",
       "      <td>3.012015</td>\n",
       "      <td>-0.544856</td>\n",
       "      <td>-0.524173</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.367741</td>\n",
       "      <td>-0.173446</td>\n",
       "      <td>-0.380127</td>\n",
       "      <td>-0.131301</td>\n",
       "      <td>-0.064531</td>\n",
       "      <td>-0.170024</td>\n",
       "      <td>-0.042360</td>\n",
       "      <td>-0.323712</td>\n",
       "      <td>2080.000000</td>\n",
       "      <td>328.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.657412e+07</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>634.000000</td>\n",
       "      <td>1.952800</td>\n",
       "      <td>3.227493</td>\n",
       "      <td>-0.133677</td>\n",
       "      <td>3.215489</td>\n",
       "      <td>3.213831</td>\n",
       "      <td>-0.090082</td>\n",
       "      <td>-0.133825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162484</td>\n",
       "      <td>0.035862</td>\n",
       "      <td>-0.028620</td>\n",
       "      <td>0.091651</td>\n",
       "      <td>0.053591</td>\n",
       "      <td>0.122405</td>\n",
       "      <td>0.004071</td>\n",
       "      <td>-0.100084</td>\n",
       "      <td>2318.000000</td>\n",
       "      <td>657.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.658695e+07</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>2.531363</td>\n",
       "      <td>3.429349</td>\n",
       "      <td>0.398711</td>\n",
       "      <td>3.415995</td>\n",
       "      <td>3.416494</td>\n",
       "      <td>0.371238</td>\n",
       "      <td>0.182307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684143</td>\n",
       "      <td>0.421848</td>\n",
       "      <td>0.621562</td>\n",
       "      <td>0.350158</td>\n",
       "      <td>0.166294</td>\n",
       "      <td>0.274269</td>\n",
       "      <td>0.072348</td>\n",
       "      <td>0.169723</td>\n",
       "      <td>2484.000000</td>\n",
       "      <td>989.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.659978e+07</td>\n",
       "      <td>1684.000000</td>\n",
       "      <td>967.000000</td>\n",
       "      <td>4.677842</td>\n",
       "      <td>4.442627</td>\n",
       "      <td>3.012100</td>\n",
       "      <td>4.383210</td>\n",
       "      <td>4.357268</td>\n",
       "      <td>2.148427</td>\n",
       "      <td>2.898562</td>\n",
       "      <td>...</td>\n",
       "      <td>2.467610</td>\n",
       "      <td>1.522810</td>\n",
       "      <td>3.424407</td>\n",
       "      <td>2.347394</td>\n",
       "      <td>0.743473</td>\n",
       "      <td>1.421510</td>\n",
       "      <td>0.356702</td>\n",
       "      <td>1.615337</td>\n",
       "      <td>2651.000000</td>\n",
       "      <td>1327.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id       date_id       time_id        weight    feature_00  \\\n",
       "count  5.000000e+04  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean   4.657321e+07   1683.684000    592.229720      1.989610      3.227522   \n",
       "std    1.592014e+04      0.464918    251.119804      0.909354      0.300719   \n",
       "min    4.654537e+07   1683.000000     68.000000      0.702595      1.952113   \n",
       "25%    4.655855e+07   1683.000000    396.000000      1.217208      3.024758   \n",
       "50%    4.657412e+07   1684.000000    634.000000      1.952800      3.227493   \n",
       "75%    4.658695e+07   1684.000000    801.000000      2.531363      3.429349   \n",
       "max    4.659978e+07   1684.000000    967.000000      4.677842      4.442627   \n",
       "\n",
       "         feature_01    feature_02    feature_03    feature_04    feature_05  \\\n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean      -0.109737      3.214889      3.214286     -0.084449     -0.217516   \n",
       "std        0.815849      0.300416      0.300179      0.640345      0.709463   \n",
       "min       -2.421601      1.915817      1.968258     -2.727498     -4.481705   \n",
       "25%       -0.664834      3.011751      3.012015     -0.544856     -0.524173   \n",
       "50%       -0.133677      3.215489      3.213831     -0.090082     -0.133825   \n",
       "75%        0.398711      3.415995      3.416494      0.371238      0.182307   \n",
       "max        3.012100      4.383210      4.357268      2.148427      2.898562   \n",
       "\n",
       "       ...  responder_0_lag_1  responder_1_lag_1  responder_2_lag_1  \\\n",
       "count  ...       50000.000000       50000.000000       50000.000000   \n",
       "mean   ...           0.139762           0.139813           0.148102   \n",
       "std    ...           0.901758           0.478356           0.881593   \n",
       "min    ...          -2.663801          -1.111050          -2.362758   \n",
       "25%    ...          -0.367741          -0.173446          -0.380127   \n",
       "50%    ...           0.162484           0.035862          -0.028620   \n",
       "75%    ...           0.684143           0.421848           0.621562   \n",
       "max    ...           2.467610           1.522810           3.424407   \n",
       "\n",
       "       responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  \\\n",
       "count       50000.000000       50000.000000       50000.000000   \n",
       "mean            0.084448           0.042563           0.089323   \n",
       "std             0.556480           0.244032           0.417234   \n",
       "min            -1.229572          -0.503404          -1.009007   \n",
       "25%            -0.131301          -0.064531          -0.170024   \n",
       "50%             0.091651           0.053591           0.122405   \n",
       "75%             0.350158           0.166294           0.274269   \n",
       "max             2.347394           0.743473           1.421510   \n",
       "\n",
       "       responder_7_lag_1  responder_8_lag_1      time_idx   time_idx_2  \n",
       "count       50000.000000       50000.000000  50000.000000  50000.00000  \n",
       "mean            0.011148          -0.088894   2275.913720    659.17028  \n",
       "std             0.101158           0.457286    250.916599    382.27245  \n",
       "min            -0.243303          -1.199404   1752.000000      0.00000  \n",
       "25%            -0.042360          -0.323712   2080.000000    328.00000  \n",
       "50%             0.004071          -0.100084   2318.000000    657.00000  \n",
       "75%             0.072348           0.169723   2484.000000    989.00000  \n",
       "max             0.356702           1.615337   2651.000000   1327.00000  \n",
       "\n",
       "[8 rows x 103 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>time_id</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>...</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>date_time</th>\n",
       "      <th>new_col</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4414087</th>\n",
       "      <td>46042217</td>\n",
       "      <td>1669</td>\n",
       "      <td>891</td>\n",
       "      <td>30</td>\n",
       "      <td>1.690181</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>-0.482614</td>\n",
       "      <td>0.244307</td>\n",
       "      <td>0.440378</td>\n",
       "      <td>1.420072</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.399602</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-0.210983</td>\n",
       "      <td>-0.118711</td>\n",
       "      <td>-0.470913</td>\n",
       "      <td>-0.019774</td>\n",
       "      <td>-0.271890</td>\n",
       "      <td>0</td>\n",
       "      <td>(1669, 891)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414088</th>\n",
       "      <td>46042218</td>\n",
       "      <td>1669</td>\n",
       "      <td>891</td>\n",
       "      <td>31</td>\n",
       "      <td>0.715662</td>\n",
       "      <td>0.540215</td>\n",
       "      <td>-0.634153</td>\n",
       "      <td>0.769046</td>\n",
       "      <td>0.571708</td>\n",
       "      <td>1.188633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.520475</td>\n",
       "      <td>0.876531</td>\n",
       "      <td>-0.101602</td>\n",
       "      <td>-0.032214</td>\n",
       "      <td>-0.674036</td>\n",
       "      <td>-0.104636</td>\n",
       "      <td>-0.739772</td>\n",
       "      <td>0</td>\n",
       "      <td>(1669, 891)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414089</th>\n",
       "      <td>46042219</td>\n",
       "      <td>1669</td>\n",
       "      <td>891</td>\n",
       "      <td>32</td>\n",
       "      <td>1.524050</td>\n",
       "      <td>0.383557</td>\n",
       "      <td>-0.567525</td>\n",
       "      <td>0.847890</td>\n",
       "      <td>0.696430</td>\n",
       "      <td>0.876198</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.186472</td>\n",
       "      <td>-0.413349</td>\n",
       "      <td>0.031786</td>\n",
       "      <td>0.020555</td>\n",
       "      <td>-0.031585</td>\n",
       "      <td>-0.008285</td>\n",
       "      <td>-0.177291</td>\n",
       "      <td>0</td>\n",
       "      <td>(1669, 891)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414090</th>\n",
       "      <td>46042220</td>\n",
       "      <td>1669</td>\n",
       "      <td>891</td>\n",
       "      <td>33</td>\n",
       "      <td>1.386579</td>\n",
       "      <td>0.198206</td>\n",
       "      <td>-0.273108</td>\n",
       "      <td>-0.561643</td>\n",
       "      <td>0.272005</td>\n",
       "      <td>0.886292</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.020017</td>\n",
       "      <td>-0.077395</td>\n",
       "      <td>0.253451</td>\n",
       "      <td>0.150415</td>\n",
       "      <td>-0.131873</td>\n",
       "      <td>-0.004737</td>\n",
       "      <td>-0.236067</td>\n",
       "      <td>0</td>\n",
       "      <td>(1669, 891)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414091</th>\n",
       "      <td>46042221</td>\n",
       "      <td>1669</td>\n",
       "      <td>891</td>\n",
       "      <td>34</td>\n",
       "      <td>2.722149</td>\n",
       "      <td>-0.174607</td>\n",
       "      <td>-0.724030</td>\n",
       "      <td>0.578088</td>\n",
       "      <td>0.710770</td>\n",
       "      <td>0.674784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047733</td>\n",
       "      <td>-0.230586</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.104809</td>\n",
       "      <td>-0.177008</td>\n",
       "      <td>0.022756</td>\n",
       "      <td>-0.076810</td>\n",
       "      <td>0</td>\n",
       "      <td>(1669, 891)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971642</th>\n",
       "      <td>46599772</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>33</td>\n",
       "      <td>1.260113</td>\n",
       "      <td>2.854824</td>\n",
       "      <td>-0.182001</td>\n",
       "      <td>3.030202</td>\n",
       "      <td>2.955101</td>\n",
       "      <td>-0.034863</td>\n",
       "      <td>...</td>\n",
       "      <td>0.680279</td>\n",
       "      <td>0.839948</td>\n",
       "      <td>0.411006</td>\n",
       "      <td>0.187644</td>\n",
       "      <td>0.273078</td>\n",
       "      <td>-0.016302</td>\n",
       "      <td>-0.182008</td>\n",
       "      <td>13576</td>\n",
       "      <td>(1684, 967)</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971643</th>\n",
       "      <td>46599773</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>34</td>\n",
       "      <td>2.364175</td>\n",
       "      <td>3.328318</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>3.429580</td>\n",
       "      <td>2.893713</td>\n",
       "      <td>0.575843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091131</td>\n",
       "      <td>-0.555067</td>\n",
       "      <td>-0.374748</td>\n",
       "      <td>-0.135764</td>\n",
       "      <td>-0.168106</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>13576</td>\n",
       "      <td>(1684, 967)</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971644</th>\n",
       "      <td>46599774</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>35</td>\n",
       "      <td>0.971465</td>\n",
       "      <td>3.334346</td>\n",
       "      <td>-0.092881</td>\n",
       "      <td>3.382863</td>\n",
       "      <td>3.111636</td>\n",
       "      <td>0.644229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.792256</td>\n",
       "      <td>-0.687760</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.033282</td>\n",
       "      <td>0.122405</td>\n",
       "      <td>0.026679</td>\n",
       "      <td>-0.025678</td>\n",
       "      <td>13576</td>\n",
       "      <td>(1684, 967)</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971646</th>\n",
       "      <td>46599776</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>37</td>\n",
       "      <td>1.306336</td>\n",
       "      <td>3.129787</td>\n",
       "      <td>-0.023663</td>\n",
       "      <td>3.379802</td>\n",
       "      <td>3.387269</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304786</td>\n",
       "      <td>-0.029890</td>\n",
       "      <td>1.104309</td>\n",
       "      <td>0.650506</td>\n",
       "      <td>0.459980</td>\n",
       "      <td>-0.007395</td>\n",
       "      <td>-0.160535</td>\n",
       "      <td>13576</td>\n",
       "      <td>(1684, 967)</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971647</th>\n",
       "      <td>46599777</td>\n",
       "      <td>1684</td>\n",
       "      <td>967</td>\n",
       "      <td>38</td>\n",
       "      <td>2.428597</td>\n",
       "      <td>3.204614</td>\n",
       "      <td>0.696010</td>\n",
       "      <td>2.955036</td>\n",
       "      <td>3.043746</td>\n",
       "      <td>0.339810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085374</td>\n",
       "      <td>0.258105</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>0.581888</td>\n",
       "      <td>0.065826</td>\n",
       "      <td>0.110606</td>\n",
       "      <td>13576</td>\n",
       "      <td>(1684, 967)</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 105 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  date_id  time_id symbol_id    weight  feature_00  \\\n",
       "4414087  46042217     1669      891        30  1.690181    0.007090   \n",
       "4414088  46042218     1669      891        31  0.715662    0.540215   \n",
       "4414089  46042219     1669      891        32  1.524050    0.383557   \n",
       "4414090  46042220     1669      891        33  1.386579    0.198206   \n",
       "4414091  46042221     1669      891        34  2.722149   -0.174607   \n",
       "...           ...      ...      ...       ...       ...         ...   \n",
       "4971642  46599772     1684      967        33  1.260113    2.854824   \n",
       "4971643  46599773     1684      967        34  2.364175    3.328318   \n",
       "4971644  46599774     1684      967        35  0.971465    3.334346   \n",
       "4971646  46599776     1684      967        37  1.306336    3.129787   \n",
       "4971647  46599777     1684      967        38  2.428597    3.204614   \n",
       "\n",
       "         feature_01  feature_02  feature_03  feature_04  ...  \\\n",
       "4414087   -0.482614    0.244307    0.440378    1.420072  ...   \n",
       "4414088   -0.634153    0.769046    0.571708    1.188633  ...   \n",
       "4414089   -0.567525    0.847890    0.696430    0.876198  ...   \n",
       "4414090   -0.273108   -0.561643    0.272005    0.886292  ...   \n",
       "4414091   -0.724030    0.578088    0.710770    0.674784  ...   \n",
       "...             ...         ...         ...         ...  ...   \n",
       "4971642   -0.182001    3.030202    2.955101   -0.034863  ...   \n",
       "4971643    0.004640    3.429580    2.893713    0.575843  ...   \n",
       "4971644   -0.092881    3.382863    3.111636    0.644229  ...   \n",
       "4971646   -0.023663    3.379802    3.387269    0.109540  ...   \n",
       "4971647    0.696010    2.955036    3.043746    0.339810  ...   \n",
       "\n",
       "         responder_1_lag_1  responder_2_lag_1  responder_3_lag_1  \\\n",
       "4414087          -0.399602          -0.504502          -0.210983   \n",
       "4414088           0.520475           0.876531          -0.101602   \n",
       "4414089          -0.186472          -0.413349           0.031786   \n",
       "4414090          -1.020017          -0.077395           0.253451   \n",
       "4414091          -0.047733          -0.230586           0.145945   \n",
       "...                    ...                ...                ...   \n",
       "4971642           0.680279           0.839948           0.411006   \n",
       "4971643           0.091131          -0.555067          -0.374748   \n",
       "4971644           0.792256          -0.687760           0.040876   \n",
       "4971646           0.304786          -0.029890           1.104309   \n",
       "4971647           0.085374           0.258105           0.077070   \n",
       "\n",
       "         responder_4_lag_1  responder_5_lag_1  responder_7_lag_1  \\\n",
       "4414087          -0.118711          -0.470913          -0.019774   \n",
       "4414088          -0.032214          -0.674036          -0.104636   \n",
       "4414089           0.020555          -0.031585          -0.008285   \n",
       "4414090           0.150415          -0.131873          -0.004737   \n",
       "4414091           0.104809          -0.177008           0.022756   \n",
       "...                    ...                ...                ...   \n",
       "4971642           0.187644           0.273078          -0.016302   \n",
       "4971643          -0.135764          -0.168106           0.041251   \n",
       "4971644           0.033282           0.122405           0.026679   \n",
       "4971646           0.650506           0.459980          -0.007395   \n",
       "4971647           0.052239           0.581888           0.065826   \n",
       "\n",
       "         responder_8_lag_1  time_idx    date_time  new_col  \n",
       "4414087          -0.271890         0  (1669, 891)        0  \n",
       "4414088          -0.739772         0  (1669, 891)        0  \n",
       "4414089          -0.177291         0  (1669, 891)        0  \n",
       "4414090          -0.236067         0  (1669, 891)        0  \n",
       "4414091          -0.076810         0  (1669, 891)        0  \n",
       "...                    ...       ...          ...      ...  \n",
       "4971642          -0.182008     13576  (1684, 967)    13576  \n",
       "4971643           0.005263     13576  (1684, 967)    13576  \n",
       "4971644          -0.025678     13576  (1684, 967)    13576  \n",
       "4971646          -0.160535     13576  (1684, 967)    13576  \n",
       "4971647           0.110606     13576  (1684, 967)    13576  \n",
       "\n",
       "[500000 rows x 105 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_86549/3419571085.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_small.drop(columns=columns_to_drop, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "columns_to_drop = [\"id\",\"date_id\",'time_id',\"date_time\"]\n",
    "df_small.drop(columns=columns_to_drop, inplace=True)\n",
    "test_df_small.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>weight</th>\n",
       "      <th>feature_00</th>\n",
       "      <th>feature_01</th>\n",
       "      <th>feature_02</th>\n",
       "      <th>feature_03</th>\n",
       "      <th>feature_04</th>\n",
       "      <th>feature_05</th>\n",
       "      <th>feature_06</th>\n",
       "      <th>feature_07</th>\n",
       "      <th>...</th>\n",
       "      <th>label</th>\n",
       "      <th>responder_0_lag_1</th>\n",
       "      <th>responder_1_lag_1</th>\n",
       "      <th>responder_2_lag_1</th>\n",
       "      <th>responder_3_lag_1</th>\n",
       "      <th>responder_4_lag_1</th>\n",
       "      <th>responder_5_lag_1</th>\n",
       "      <th>responder_7_lag_1</th>\n",
       "      <th>responder_8_lag_1</th>\n",
       "      <th>time_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4414087</th>\n",
       "      <td>30</td>\n",
       "      <td>1.690181</td>\n",
       "      <td>0.007090</td>\n",
       "      <td>-0.482614</td>\n",
       "      <td>0.244307</td>\n",
       "      <td>0.440378</td>\n",
       "      <td>1.420072</td>\n",
       "      <td>0.640939</td>\n",
       "      <td>-0.090522</td>\n",
       "      <td>1.332146</td>\n",
       "      <td>...</td>\n",
       "      <td>-4</td>\n",
       "      <td>-0.357139</td>\n",
       "      <td>-0.399602</td>\n",
       "      <td>-0.504502</td>\n",
       "      <td>-0.210983</td>\n",
       "      <td>-0.118711</td>\n",
       "      <td>-0.470913</td>\n",
       "      <td>-0.019774</td>\n",
       "      <td>-0.271890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414088</th>\n",
       "      <td>31</td>\n",
       "      <td>0.715662</td>\n",
       "      <td>0.540215</td>\n",
       "      <td>-0.634153</td>\n",
       "      <td>0.769046</td>\n",
       "      <td>0.571708</td>\n",
       "      <td>1.188633</td>\n",
       "      <td>0.720958</td>\n",
       "      <td>-0.158161</td>\n",
       "      <td>1.120538</td>\n",
       "      <td>...</td>\n",
       "      <td>-3</td>\n",
       "      <td>1.135520</td>\n",
       "      <td>0.520475</td>\n",
       "      <td>0.876531</td>\n",
       "      <td>-0.101602</td>\n",
       "      <td>-0.032214</td>\n",
       "      <td>-0.674036</td>\n",
       "      <td>-0.104636</td>\n",
       "      <td>-0.739772</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414089</th>\n",
       "      <td>32</td>\n",
       "      <td>1.524050</td>\n",
       "      <td>0.383557</td>\n",
       "      <td>-0.567525</td>\n",
       "      <td>0.847890</td>\n",
       "      <td>0.696430</td>\n",
       "      <td>0.876198</td>\n",
       "      <td>0.443820</td>\n",
       "      <td>-0.097515</td>\n",
       "      <td>0.714503</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.295801</td>\n",
       "      <td>-0.186472</td>\n",
       "      <td>-0.413349</td>\n",
       "      <td>0.031786</td>\n",
       "      <td>0.020555</td>\n",
       "      <td>-0.031585</td>\n",
       "      <td>-0.008285</td>\n",
       "      <td>-0.177291</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414090</th>\n",
       "      <td>33</td>\n",
       "      <td>1.386579</td>\n",
       "      <td>0.198206</td>\n",
       "      <td>-0.273108</td>\n",
       "      <td>-0.561643</td>\n",
       "      <td>0.272005</td>\n",
       "      <td>0.886292</td>\n",
       "      <td>0.529212</td>\n",
       "      <td>-0.084445</td>\n",
       "      <td>1.192398</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.134588</td>\n",
       "      <td>-1.020017</td>\n",
       "      <td>-0.077395</td>\n",
       "      <td>0.253451</td>\n",
       "      <td>0.150415</td>\n",
       "      <td>-0.131873</td>\n",
       "      <td>-0.004737</td>\n",
       "      <td>-0.236067</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4414091</th>\n",
       "      <td>34</td>\n",
       "      <td>2.722149</td>\n",
       "      <td>-0.174607</td>\n",
       "      <td>-0.724030</td>\n",
       "      <td>0.578088</td>\n",
       "      <td>0.710770</td>\n",
       "      <td>0.674784</td>\n",
       "      <td>1.204417</td>\n",
       "      <td>-0.221221</td>\n",
       "      <td>1.627897</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.075389</td>\n",
       "      <td>-0.047733</td>\n",
       "      <td>-0.230586</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.104809</td>\n",
       "      <td>-0.177008</td>\n",
       "      <td>0.022756</td>\n",
       "      <td>-0.076810</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971642</th>\n",
       "      <td>33</td>\n",
       "      <td>1.260113</td>\n",
       "      <td>2.854824</td>\n",
       "      <td>-0.182001</td>\n",
       "      <td>3.030202</td>\n",
       "      <td>2.955101</td>\n",
       "      <td>-0.034863</td>\n",
       "      <td>0.020673</td>\n",
       "      <td>0.583853</td>\n",
       "      <td>0.127877</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.421281</td>\n",
       "      <td>0.680279</td>\n",
       "      <td>0.839948</td>\n",
       "      <td>0.411006</td>\n",
       "      <td>0.187644</td>\n",
       "      <td>0.273078</td>\n",
       "      <td>-0.016302</td>\n",
       "      <td>-0.182008</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971643</th>\n",
       "      <td>34</td>\n",
       "      <td>2.364175</td>\n",
       "      <td>3.328318</td>\n",
       "      <td>0.004640</td>\n",
       "      <td>3.429580</td>\n",
       "      <td>2.893713</td>\n",
       "      <td>0.575843</td>\n",
       "      <td>0.044370</td>\n",
       "      <td>1.525104</td>\n",
       "      <td>0.242329</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.557703</td>\n",
       "      <td>0.091131</td>\n",
       "      <td>-0.555067</td>\n",
       "      <td>-0.374748</td>\n",
       "      <td>-0.135764</td>\n",
       "      <td>-0.168106</td>\n",
       "      <td>0.041251</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971644</th>\n",
       "      <td>35</td>\n",
       "      <td>0.971465</td>\n",
       "      <td>3.334346</td>\n",
       "      <td>-0.092881</td>\n",
       "      <td>3.382863</td>\n",
       "      <td>3.111636</td>\n",
       "      <td>0.644229</td>\n",
       "      <td>0.016599</td>\n",
       "      <td>0.837059</td>\n",
       "      <td>0.181541</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.923129</td>\n",
       "      <td>0.792256</td>\n",
       "      <td>-0.687760</td>\n",
       "      <td>0.040876</td>\n",
       "      <td>0.033282</td>\n",
       "      <td>0.122405</td>\n",
       "      <td>0.026679</td>\n",
       "      <td>-0.025678</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971646</th>\n",
       "      <td>37</td>\n",
       "      <td>1.306336</td>\n",
       "      <td>3.129787</td>\n",
       "      <td>-0.023663</td>\n",
       "      <td>3.379802</td>\n",
       "      <td>3.387269</td>\n",
       "      <td>0.109540</td>\n",
       "      <td>0.028464</td>\n",
       "      <td>1.360380</td>\n",
       "      <td>0.178313</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.568908</td>\n",
       "      <td>0.304786</td>\n",
       "      <td>-0.029890</td>\n",
       "      <td>1.104309</td>\n",
       "      <td>0.650506</td>\n",
       "      <td>0.459980</td>\n",
       "      <td>-0.007395</td>\n",
       "      <td>-0.160535</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971647</th>\n",
       "      <td>38</td>\n",
       "      <td>2.428597</td>\n",
       "      <td>3.204614</td>\n",
       "      <td>0.696010</td>\n",
       "      <td>2.955036</td>\n",
       "      <td>3.043746</td>\n",
       "      <td>0.339810</td>\n",
       "      <td>0.027520</td>\n",
       "      <td>1.561544</td>\n",
       "      <td>0.198252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.324545</td>\n",
       "      <td>0.085374</td>\n",
       "      <td>0.258105</td>\n",
       "      <td>0.077070</td>\n",
       "      <td>0.052239</td>\n",
       "      <td>0.581888</td>\n",
       "      <td>0.065826</td>\n",
       "      <td>0.110606</td>\n",
       "      <td>13576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        symbol_id    weight  feature_00  feature_01  feature_02  feature_03  \\\n",
       "4414087        30  1.690181    0.007090   -0.482614    0.244307    0.440378   \n",
       "4414088        31  0.715662    0.540215   -0.634153    0.769046    0.571708   \n",
       "4414089        32  1.524050    0.383557   -0.567525    0.847890    0.696430   \n",
       "4414090        33  1.386579    0.198206   -0.273108   -0.561643    0.272005   \n",
       "4414091        34  2.722149   -0.174607   -0.724030    0.578088    0.710770   \n",
       "...           ...       ...         ...         ...         ...         ...   \n",
       "4971642        33  1.260113    2.854824   -0.182001    3.030202    2.955101   \n",
       "4971643        34  2.364175    3.328318    0.004640    3.429580    2.893713   \n",
       "4971644        35  0.971465    3.334346   -0.092881    3.382863    3.111636   \n",
       "4971646        37  1.306336    3.129787   -0.023663    3.379802    3.387269   \n",
       "4971647        38  2.428597    3.204614    0.696010    2.955036    3.043746   \n",
       "\n",
       "         feature_04  feature_05  feature_06  feature_07  ...  label  \\\n",
       "4414087    1.420072    0.640939   -0.090522    1.332146  ...     -4   \n",
       "4414088    1.188633    0.720958   -0.158161    1.120538  ...     -3   \n",
       "4414089    0.876198    0.443820   -0.097515    0.714503  ...     -1   \n",
       "4414090    0.886292    0.529212   -0.084445    1.192398  ...      2   \n",
       "4414091    0.674784    1.204417   -0.221221    1.627897  ...      0   \n",
       "...             ...         ...         ...         ...  ...    ...   \n",
       "4971642   -0.034863    0.020673    0.583853    0.127877  ...      0   \n",
       "4971643    0.575843    0.044370    1.525104    0.242329  ...      0   \n",
       "4971644    0.644229    0.016599    0.837059    0.181541  ...      0   \n",
       "4971646    0.109540    0.028464    1.360380    0.178313  ...      0   \n",
       "4971647    0.339810    0.027520    1.561544    0.198252  ...      0   \n",
       "\n",
       "         responder_0_lag_1  responder_1_lag_1  responder_2_lag_1  \\\n",
       "4414087          -0.357139          -0.399602          -0.504502   \n",
       "4414088           1.135520           0.520475           0.876531   \n",
       "4414089          -0.295801          -0.186472          -0.413349   \n",
       "4414090          -2.134588          -1.020017          -0.077395   \n",
       "4414091          -0.075389          -0.047733          -0.230586   \n",
       "...                    ...                ...                ...   \n",
       "4971642           1.421281           0.680279           0.839948   \n",
       "4971643           0.557703           0.091131          -0.555067   \n",
       "4971644           0.923129           0.792256          -0.687760   \n",
       "4971646           0.568908           0.304786          -0.029890   \n",
       "4971647          -0.324545           0.085374           0.258105   \n",
       "\n",
       "         responder_3_lag_1  responder_4_lag_1  responder_5_lag_1  \\\n",
       "4414087          -0.210983          -0.118711          -0.470913   \n",
       "4414088          -0.101602          -0.032214          -0.674036   \n",
       "4414089           0.031786           0.020555          -0.031585   \n",
       "4414090           0.253451           0.150415          -0.131873   \n",
       "4414091           0.145945           0.104809          -0.177008   \n",
       "...                    ...                ...                ...   \n",
       "4971642           0.411006           0.187644           0.273078   \n",
       "4971643          -0.374748          -0.135764          -0.168106   \n",
       "4971644           0.040876           0.033282           0.122405   \n",
       "4971646           1.104309           0.650506           0.459980   \n",
       "4971647           0.077070           0.052239           0.581888   \n",
       "\n",
       "         responder_7_lag_1  responder_8_lag_1  time_idx  \n",
       "4414087          -0.019774          -0.271890         0  \n",
       "4414088          -0.104636          -0.739772         0  \n",
       "4414089          -0.008285          -0.177291         0  \n",
       "4414090          -0.004737          -0.236067         0  \n",
       "4414091           0.022756          -0.076810         0  \n",
       "...                    ...                ...       ...  \n",
       "4971642          -0.016302          -0.182008     13576  \n",
       "4971643           0.041251           0.005263     13576  \n",
       "4971644           0.026679          -0.025678     13576  \n",
       "4971646          -0.007395          -0.160535     13576  \n",
       "4971647           0.065826           0.110606     13576  \n",
       "\n",
       "[500000 rows x 100 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_varying_unknown_reals = df_small.drop(columns=[\"label\",\"symbol_id\",\"time_idx\"]).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:572: UserWarning: Target scales will be only added for continous targets\n",
      "  warnings.warn(\"Target scales will be only added for continous targets\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "training = TimeSeriesDataSet(\n",
    "    df_small,\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"label\",\n",
    "    group_ids=[\"symbol_id\"],\n",
    "    min_encoder_length=max_encoder_length // 2,  # allow predictions without history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_idx=0,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"symbol_id\"],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[],\n",
    "    time_varying_unknown_reals=time_varying_unknown_reals,\n",
    "    #target_normalizer=GroupNormalizer(transformation=\"softplus\"),\n",
    "    add_relative_time_idx=True,\n",
    "    add_target_scales=True,\n",
    "    allow_missing_timesteps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = training.to_dataloader(train=True, batch_size=64, num_workers=0)\n",
    "\n",
    "validation = TimeSeriesDataSet.from_dataset(training, df_small, predict=True, stop_randomization=True)\n",
    "val_dataloader = validation.to_dataloader(train=False, batch_size=64 * 10, num_workers=0)\n",
    "\n",
    "test_dataloader = TimeSeriesDataSet.from_dataset(training, test_df_small,categorical_encoders={\"symbol_id\": NaNLabelEncoder(add_nan=True)},predict=True, stop_randomization=True).to_dataloader(train=False, batch_size=64 * 10, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_86549/296535705.py:1: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5737)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_predictions = Baseline().predict(val_dataloader, return_y=True)\n",
    "MAE()(baseline_predictions.output, baseline_predictions.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "trainer = pl.Trainer(\n",
    "    #accelerator=\"cpu\",\n",
    "    # clipping gradients is a hyperparameter and important to prevent divergance\n",
    "    # of the gradient for recurrent neural networks\n",
    "    gradient_clip_val=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:143: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 84.1k\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    #hidden_size=8, \n",
    "    #attention_head_size=4,\n",
    "    #dropout=0.1,\n",
    "    #hidden_continuous_size=8,\n",
    "    #loss=RMSE(),\n",
    "    #optimizer=\"Ranger\",\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# configure network and trainer\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"lightning_logs\")  # logging results to a tensorboard\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=10,\n",
    "    accelerator=\"cpu\",\n",
    "    enable_model_summary=True,\n",
    "    gradient_clip_val=0.1,\n",
    "    limit_train_batches=50,  # coment in for training, running valiation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "    callbacks=[lr_logger, early_stop_callback],\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/loops/training_epoch_loop.py:161: You're resuming from a checkpoint that ended before the epoch ended and your dataloader is not resumable. This can cause unreliable results if further training is done. Consider using an end-of-epoch checkpoint or make your dataloader resumable by implementing the `state_dict` / `load_state_dict` interface.\n",
      "Finding best initial lr:   1%|          | 1/100 [00:00<00:53,  1.86it/s]\n",
      "LR finder stopped early after 35 steps due to diverging loss.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Restoring states from the checkpoint path at /Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/code/.lr_find_6e5db7ea-88d1-44b1-8b7b-78ae0d40cff1.ckpt\n",
      "Restored all states from the checkpoint at /Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/code/.lr_find_6e5db7ea-88d1-44b1-8b7b-78ae0d40cff1.ckpt\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n",
      "Failed to compute suggestion for learning rate because there are not enough points. Increase the loop iteration limits or the size of your dataset/dataloader.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suggested learning rate: None\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/ddz0qbqx0_30722ndkqpl39r0000gn/T/ipykernel_86549/3629428528.py:12: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n",
      "  fig.show()\n"
     ]
    }
   ],
   "source": [
    "#res = Tuner(trainer).lr_find(\n",
    "#    tft,\n",
    "#    train_dataloaders=train_dataloader,\n",
    "#    val_dataloaders=val_dataloader,\n",
    "#    max_lr=1,\n",
    "#    min_lr=1e-4,\n",
    "#    num_training=100,\n",
    "#)\n",
    "\n",
    "#print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "#fig = res.plot(show=True, suggest=True)\n",
    "#fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/utilities/parsing.py:208: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:143: UserWarning: In pytorch-forecasting models, on versions 1.1.X, the default optimizer defaults to 'adam', if pytorch_optimizer is not installed, otherwise it defaults to 'ranger' from pytorch_optimizer. From version 1.2.0, the default optimizer will be 'adam' regardless of whether pytorch_optimizer is installed, in order to minimize the number of dependencies in default parameter settings. Users who wish to ensure their code continues using 'ranger' as optimizer should ensure that pytorch_optimizer is installed, and set the optimizer parameter explicitly to 'ranger'.\n",
      "  super().__init__(loss=loss, logging_metrics=logging_metrics, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in network: 84.1k\n"
     ]
    }
   ],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.030199517204020164,\n",
    "    #hidden_size=16,\n",
    "    #attention_head_size=4,\n",
    "    #dropout=0.1,\n",
    "    #hidden_continuous_size=8,\n",
    "    #loss=RMSE(),\n",
    "    #log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "    #optimizer=\"Ranger\",\n",
    "    #reduce_on_plateau_patience=4,\n",
    "    #output_size = 21,\n",
    ")\n",
    "print(f\"Number of parameters in network: {tft.size() / 1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "   | Name                               | Type                            | Params | Mode \n",
      "------------------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0      | train\n",
      "1  | logging_metrics                    | ModuleList                      | 0      | train\n",
      "2  | input_embeddings                   | MultiEmbedding                  | 456    | train\n",
      "3  | prescalers                         | ModuleDict                      | 1.6 K  | train\n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 646    | train\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 68.7 K | train\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 528    | train\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K  | train\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K  | train\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K  | train\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K  | train\n",
      "11 | lstm_encoder                       | LSTM                            | 2.2 K  | train\n",
      "12 | lstm_decoder                       | LSTM                            | 2.2 K  | train\n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544    | train\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 32     | train\n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K  | train\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 676    | train\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 576    | train\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K  | train\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 576    | train\n",
      "20 | output_layer                       | Linear                          | 119    | train\n",
      "------------------------------------------------------------------------------------------------\n",
      "84.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "84.1 K    Total params\n",
      "0.336     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paul-antoine/Desktop/ENSAE/Advanced-ML-Project/.venv/lib/python3.8/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 50/50 [00:31<00:00,  1.58it/s, v_num=20, train_loss_step=0.454, val_loss=0.335, train_loss_epoch=0.439]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 50/50 [00:32<00:00,  1.56it/s, v_num=20, train_loss_step=0.454, val_loss=0.335, train_loss_epoch=0.439]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
