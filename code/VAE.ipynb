{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import xgboost as xgb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.adam import Adam\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import joblib\n",
    "\n",
    "from suave import SuaveClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the parquet files in the data folder\n",
    "full_data = pd.read_parquet('../data/train-0.parquet')\n",
    "full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the data into features and target\n",
    "def data_preprocess(data, max_lag = 3, symbol=None):\n",
    "\n",
    "    if symbol is not None:\n",
    "        # Filter data for the selected symbol\n",
    "        df = data[data['symbol_id'] == symbol]\n",
    "    else:\n",
    "        df = data\n",
    "\n",
    "    # Create lags\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        df[f'responder_6_lag{lag}'] = df.groupby('symbol_id')['responder_6'].shift(lag)\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    df.dropna(subset=[f'responder_6_lag{lag}' for lag in range(1, max_lag+1)], inplace=True)\n",
    "\n",
    "    # Drop columns with more than 50% missing values\n",
    "    missing_values = df.isnull().mean()\n",
    "    missing_values = missing_values[missing_values > 0.5]\n",
    "    df.drop(columns=missing_values.index, inplace=True)\n",
    "\n",
    "    # Drop columns not needed\n",
    "    exclude_cols = ['date_id', 'time_id'] + [f'responder_{i}' for i in range(9) if i != 6]\n",
    "    df.drop(columns=exclude_cols, inplace=True)\n",
    "\n",
    "    # Fill missing values with previous and next values\n",
    "    df.fillna(method='ffill', inplace=True)\n",
    "    df.fillna(method='bfill', inplace=True)\n",
    "\n",
    "    # Drop columns with zero standard deviation\n",
    "    std = df.std()\n",
    "    exclude_cols = std[std == 0].index.tolist()\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=exclude_cols + ['responder_6'])\n",
    "    y = df['responder_6']\n",
    "\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of features importances and model sensitivity to symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(symbol, data):\n",
    "    # Define columns to exclude\n",
    "    exclude_cols = ['date_id', 'time_id'] + [f'responder_{i}' for i in range(0, 9)]\n",
    "\n",
    "    # Filter data for the selected symbol\n",
    "    df = data[data['symbol_id'] == symbol]\n",
    "\n",
    "    # Create lags\n",
    "    for lag in [1, 2, 3]:\n",
    "        df[f'responder_6_lag{lag}'] = df['responder_6'].shift(lag)\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    df = df.dropna(subset=[f'responder_6_lag{lag}' for lag in [1, 2, 3]])\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=exclude_cols)\n",
    "    y = df['responder_6']\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def train_model_xgboost(symbol, data):\n",
    "    # Data preprocessing\n",
    "    X, y = data_preprocess(symbol, data)\n",
    "    X['symbol_id'] = X['symbol_id'].astype('category')\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Convert to DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "\n",
    "    # Training of a XGBoost model with verbose evaluation\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'rmse',\n",
    "        'seed': 42\n",
    "    }\n",
    "    evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "    model = xgb.train(params, dtrain, num_boost_round=1000, evals=evals, early_stopping_rounds=10, verbose_eval=10)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(dtest)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Final MSE: {mse:.2f}\")\n",
    "\n",
    "    return model, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create as many xgboost models as there are symbols and compare the results\n",
    "\n",
    "mse = {}\n",
    "features_importances = {}\n",
    "symbols = full_data['symbol_id'].unique().tolist()\n",
    "    \n",
    "for symbol in symbols:\n",
    "    print(f\"Predictions for symbol {symbol}\")\n",
    "\n",
    "    # Train XGBoost model and evaluate rmse\n",
    "    model, mse[f'symbol_{symbol}'] = train_model_xgboost(symbol, full_data)\n",
    "\n",
    "    # save the feature importances\n",
    "    importances = model.get_score(importance_type='weight')\n",
    "    features_names = model.get_score(importance_type='weight').keys()\n",
    "\n",
    "    for features in features_names:\n",
    "        if features in features_importances:\n",
    "            features_importances[f'{features}'].append(importances[features])\n",
    "        else:\n",
    "            features_importances[f'{features}'] = [importances[features]]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Plot RMSE Bar Chart and Feature Importance Boxplot\n",
    "\n",
    "# RMSE Bar Chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(mse.keys(), mse.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Symbol')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE for each symbol')\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance Boxplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot(features_importances.values())\n",
    "plt.xticks(range(1, len(features_importances) + 1), features_importances.keys(), rotation=90)\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance for each symbol')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class KaggleDataset which is a child of Dataset class from torch.util.data\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class KaggleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing the data\n",
    "        \"\"\"\n",
    "        # Store the preprocessed DataFrame in self.dataframe (can parametrize single or all symbols in data_preprocess)\n",
    "        X, y = data_preprocess(df)\n",
    "        self.dataframe = pd.concat([X, y], axis=1)\n",
    "\n",
    "        # Extract the features for easier manipulation\n",
    "        self.features = self.dataframe.values\n",
    "\n",
    "        # Calculate mean and std for normalization\n",
    "        self.mean = self.features.mean(axis=0)\n",
    "        self.std = self.features.std(axis=0)\n",
    "\n",
    "        # Apply normalization to features\n",
    "        self.features = (self.features - self.mean) / self.std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Return the item at index idx in the form of tensor\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32).to(device)\n",
    "        return features\n",
    "\n",
    "# Create a KaggleDataset object\n",
    "dataset = KaggleDataset(full_data)\n",
    "\n",
    "# Then, the batch_size and input_dim were set. The dataset was divided into train and test datasets. Each of these was loaded into a DataLoader. The device was set to ‘cuda’ if available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batchsize and input dimensions\n",
    "batch_size = 64\n",
    "input_dim = dataset.features.shape[1]\n",
    "latent_dim = input_dim // 2\n",
    "hidden_dim = (latent_dim + input_dim) // 2\n",
    "# Define learning rate\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Split dataset into train and test without shuffle in the ratio of 80:20\n",
    "train_dataset, test_dataset = random_split(dataset, [0.8,0.2])\n",
    "\n",
    "# Use DataLoader for batching and shuffling\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, device=device):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # Latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, 1)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 1)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "            )\n",
    "        \n",
    "    # Encode function\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, log_var = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, log_var\n",
    "    \n",
    "    # Add Reparameterization\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    # Decode function\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    # Forward Function\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.encode(x)\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, log_var\n",
    "    \n",
    "    # Reconstruct input from compressed form\n",
    "    def reconstruction(self, mean, log_var):\n",
    "        z = self.reparameterization(mean, log_var)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    # Reproduction Loss\n",
    "    reproduction_loss = nn.functional.mse_loss(x_hat, x)\n",
    "    # KL Divergence Loss\n",
    "    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    return reproduction_loss + KLD\n",
    "\n",
    "# VAE Model created and stored in device\n",
    "model = VAE(input_dim = input_dim,hidden_dim=hidden_dim,  latent_dim=latent_dim).to(device)\n",
    "\n",
    "# Optimizer defined\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, device):\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Loop for each epoch\n",
    "    for epoch in range(epochs):\n",
    "        overall_loss = 0\n",
    "        \n",
    "        # Iterate over the batches formed by DataLoader\n",
    "        for batch_idx, x in enumerate(train_dataloader):\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # Reset Gradient\n",
    "            optimizer.zero_grad()\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            \n",
    "            # Calculate batch loss and then overall loss\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            overall_loss += loss.item()\n",
    "            \n",
    "            # Backpropagate the loss and train the optimizer\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Loss: \", overall_loss/(batch_idx*batch_size))\n",
    "    return overall_loss\n",
    "\n",
    "# Train the model for 5 epochs\n",
    "train(model, optimizer, epochs=5, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_per_epoch = 37.1 / 5\n",
    "s_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latent representation of the data\n",
    "X_train_latent = model.encoder(torch.tensor(train_dataset.dataset.features, dtype=torch.float32).to(device))\n",
    "X_test_latent = model.encoder(torch.tensor(test_dataset.dataset.features, dtype=torch.float32).to(device))\n",
    "\n",
    "# Fit a linear regression model on the latent representation\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_latent.detach().numpy(), train_dataset.dataset.dataframe['responder_6'])\n",
    "\n",
    "# Predict the target using the linear model\n",
    "y_pred = linear_model.predict(X_test_latent.detach().numpy())\n",
    "y_true = test_dataset.dataset.dataframe['responder_6']\n",
    "\n",
    "# Calculate the mean squared error\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"MAE: {mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "# Build a cross-validation grid search for our VAE model\n",
    "\n",
    "# Define the hyperparameters to search\n",
    "hyperparameters = {\n",
    "    'hidden_dim': [input_dim - 1 , input_dim - 3],\n",
    "    'latent_dim': [input_dim - 10, input_dim - 20],\n",
    "    'learning_rate': [1e-3, 1e-4],\n",
    "    'batch_size': [64, 128]\n",
    "}\n",
    "\n",
    "# Define the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Define the number of folds for cross-validation\n",
    "n_splits = 2\n",
    "\n",
    "# Create a list of all hyperparameter combinations\n",
    "keys, values = zip(*hyperparameters.items())\n",
    "combinations = [dict(zip(keys, v)) for v in product(*values)]\n",
    "\n",
    "# Create a list of models and their corresponding hyperparameters\n",
    "models = []\n",
    "for combination in combinations:\n",
    "    model = VAE(input_dim=input_dim, hidden_dim=combination['hidden_dim'], latent_dim=combination['latent_dim']).to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=combination['learning_rate'])\n",
    "    models.append((model, optimizer, combination))\n",
    "\n",
    "# Cross-validation for each model\n",
    "for model, optimizer, combination in models:\n",
    "    print(f\"Training model with hyperparameters: {combination}\")\n",
    "    for fold, (train_idx, test_idx) in enumerate(TimeSeriesSplit(n_splits=n_splits).split(dataset)):\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=combination['batch_size'], shuffle=False)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=combination['batch_size'], shuffle=False)\n",
    "        train(model, optimizer, epochs, device=device)\n",
    "        mean, var = predict(model, test_dataloader)\n",
    "        print(f\"Model with hyperparameters {combination} and fold {fold} has been trained and evaluated.\")\n",
    "        print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost based model on all symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single xgboost model for all symbols\n",
    "max_lag = 3\n",
    "\n",
    "# create the lagged responder_6 columns\n",
    "for lag in range(1, max_lag + 1):\n",
    "    full_data[f'responder_6_lag{lag}'] = full_data.groupby('symbol_id')['responder_6'].shift(lag)\n",
    "\n",
    "# drop rows with NaN\n",
    "full_data.dropna(subset=[f'responder_6_lag{lag}' for lag in range(1, max_lag + 1)])\n",
    "\n",
    "# separate features and target\n",
    "\n",
    "exclude_cols = ['date_id', 'time_id'] + [f'responder_{i}' for i in range(0, 9)]\n",
    "X = full_data.drop(columns=exclude_cols)\n",
    "y = full_data['responder_6']\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# convert to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test, enable_categorical=True)\n",
    "\n",
    "# train the model\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 42\n",
    "}\n",
    "evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "model = xgb.train(params, dtrain, num_boost_round=1000, evals=evals, early_stopping_rounds=10, verbose_eval=10)\n",
    "\n",
    "# predict the test set\n",
    "y_pred = model.predict(dtest)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Final RMSE: {rmse:.2f}\")\n",
    "\n",
    "# plot feature importance in ascending order\n",
    "sorted_importances = {k: v for k, v in sorted(importances.items(), key=lambda item: item[1])}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xticks(rotation=90)\n",
    "plt.bar(sorted_importances.keys(), sorted_importances.values())\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()\n",
    "\n",
    "# plot predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Responder 6')\n",
    "plt.title('Predictions vs Actual Values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGboost benchmarking differents dimension reduction methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data X and y preprocessing\n",
    "X,y = data_preprocess(full_data.copy(), symbol=0)\n",
    "\n",
    "# Normaliser les données\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# dimensionality reduction with PCA, VAE and Autoencoder\n",
    "dim = X.shape[1] // 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim, hidden_dim=64):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_reconstructed = self.decoder(z)\n",
    "        return x_reconstructed, z\n",
    "\n",
    "    def train_model(self, training_set, num_epochs = 10, batch_size=64):  \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch {epoch + 1}\")\n",
    "            self.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            X_reconstructed, X_latent = self(training_set)\n",
    "            loss = self.criterion(X_reconstructed, training_set)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "autoencoder = Autoencoder(input_dim=X_train.shape[1], latent_dim=dim)\n",
    "autoencoder.train_model(torch.tensor(X_train, dtype=torch.float32))\n",
    "\n",
    "# save the model\n",
    "torch.save(autoencoder, 'autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "\n",
    "# Fonction pour réduire la dimensionnalité et retourner les nouvelles données d'entrainement et de test\n",
    "def reduce_dimensionality(X_train, X_test, dim, method):\n",
    "    if method == 'pca':\n",
    "        pca = PCA(n_components=dim)\n",
    "        X_train_reduced = pca.fit_transform(X_train)\n",
    "        X_test_reduced = pca.transform(X_test)\n",
    "    elif method == 'vae':\n",
    "        vae = joblib.load(\"suave-model.pkl\")\n",
    "        X_train_reduced = vae.transform(X_train)\n",
    "        X_test_reduced = vae.transform(X_test)\n",
    "    elif method == 'autoencoder':\n",
    "        # load the model\n",
    "        autoencoder = torch.load('autoencoder.pth')\n",
    "        with torch.no_grad():\n",
    "            _, X_train_latent = autoencoder(torch.tensor(X_train, dtype=torch.float32))\n",
    "            _, X_test_latent = autoencoder(torch.tensor(X_test, dtype=torch.float32))\n",
    "        X_train_reduced = X_train_latent.numpy()\n",
    "        X_test_reduced = X_test_latent.numpy()\n",
    "    else:\n",
    "        X_train_reduced, X_test_reduced = X_train, X_test\n",
    "    \n",
    "    return X_train_reduced, X_test_reduced\n",
    "\n",
    "# Fonction pour évaluer les performances du modèle\n",
    "def evaluate_model(X_train, X_test, y_train, y_test, model, method=None):\n",
    "    start = time.time()\n",
    "    X_train_reduced, X_test_reduced = reduce_dimensionality(X_train, X_test, dim, method)\n",
    "    model.fit(X_train_reduced, y_train)\n",
    "    end = time.time()\n",
    "    \n",
    "    y_pred = model.predict(X_test_reduced)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    training_time = end - start\n",
    "    \n",
    "    return mse, mae, training_time\n",
    "\n",
    "# Définir les modèles à évaluer\n",
    "models = {\n",
    "    'xgb': xgb.XGBRegressor(),\n",
    "    'linear': LinearRegression()\n",
    "}\n",
    "\n",
    "# Create a graphic table that shows the MSE and time_complexity Benchmark for each model and each dimensionality reduction method\n",
    "# Create a DataFrame to store the results\n",
    "results = pd.DataFrame(columns=['Model', 'Method', 'MSE', 'MAE', 'Time Complexity'])\n",
    "\n",
    "for name, model in models.items():\n",
    "    mse, mae, time_ = evaluate_model(X_train, X_test, y_train, y_test, model)\n",
    "    mse_pca, mae_pca, time_pca = evaluate_model(X_train, X_test, y_train, y_test, model, method='pca')\n",
    "    mse_autoencoder, mae_autoencoder, time_autoencoder = evaluate_model(X_train, X_test, y_train, y_test, model, method='autoencoder')\n",
    "    mse_vae, mae_vae, time_vae = evaluate_model(X_train, X_test, y_train, y_test, model, method='vae')\n",
    "    # concatenate the results\n",
    "    results = pd.concat([results, pd.DataFrame({'Model': [name]*4, 'Method': ['None', 'PCA', 'Autoencoder', 'VAE'], 'MSE': [mse, mse_pca, mse_autoencoder, mse_vae], 'MAE': [mae, mae_pca, mae_autoencoder, mae_vae], 'Time Complexity': [time_, time_pca, time_autoencoder, time_vae]})])\n",
    "    print(f\"Model {name} has MSE {mse:.2f}, MAE {mae:.2f} and Time Complexity {time_:.2f} without dimensionality reduction\")\n",
    "    print(f\"Model {name} has MSE {mse_pca:.2f}, MAE {mae_pca:.2f} and Time Complexity {time_pca:.2f} with PCA\")\n",
    "    print(f\"Model {name} has MSE {mse_autoencoder:.2f}, MAE {mae_autoencoder:.2f} and Time Complexity {time_autoencoder:.2f} with Autoencoder\")\n",
    "    print(f\"Model {name} has MSE {mse_vae:.2f}, MAE {mae_vae:.2f} and Time Complexity {time_vae:.2f} with VAE\")\n",
    "    print(\"------------------------------------------------------------------------\")\n",
    "\n",
    "# Evaluate the models and their performance in mse and time complexity with multiple bar charts aside\n",
    "# A color for each model\n",
    "colors = ['green','blue']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
    "\n",
    "# Create a bar chart for MSE\n",
    "for i, model in enumerate(models.keys()):\n",
    "    mse = results[results['Model'] == model]['MSE']\n",
    "    method = results[results['Model'] == model]['Method']\n",
    "    ax[0].bar(method, mse, color=colors[i], label=model,alpha=1-0.5*i)\n",
    "\n",
    "# Set the title and labels\n",
    "ax[0].set_title('MSE for each model and method')\n",
    "ax[0].set_xlabel('Method')\n",
    "ax[0].set_ylabel('MSE')\n",
    "ax[0].legend()\n",
    "\n",
    "\n",
    "# Create a bar chart for MAE\n",
    "for i, model in enumerate(models.keys()):\n",
    "    mae = results[results['Model'] == model]['MAE']\n",
    "    method = results[results['Model'] == model]['Method']\n",
    "    ax[1].bar(method, mae, color=colors[i], label=model, alpha=1-0.5*i)\n",
    "\n",
    "# Set the title and labels\n",
    "ax[1].set_title('MAE for each model and method')\n",
    "ax[1].set_xlabel('Method')\n",
    "ax[1].set_ylabel('MAE')\n",
    "ax[1].legend()\n",
    "\n",
    "# Create a bar chart for Time Complexity\n",
    "for i, model in enumerate(models.keys()):\n",
    "    time_complexity = results[results['Model'] == model]['Time Complexity']\n",
    "    method = results[results['Model'] == model]['Method']\n",
    "    ax[2].bar(method, time_complexity, color=colors[i], label=model, alpha=1-0.5*i)\n",
    "\n",
    "# Set the title and labels\n",
    "ax[2].set_title('Time Complexity for each model and method')\n",
    "ax[2].set_xlabel('Method')\n",
    "ax[2].set_ylabel('Time Complexity')\n",
    "ax[2].legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUAVE-ML VAE\n",
    "\n",
    "#### SUAVE: Supervised and Unified Analysis of Variational Embeddings\n",
    "\n",
    "SUAVE is a Python package built upon a Hybrid Variational Autoencoder (VAE) . It unifies unsupervised latent representation learning with supervised prediction tasks:\n",
    "\n",
    "- Supervised Learning : Utilizes VAE to map high-dimensional input features to a low-dimensional, independent latent space. This approach not only retains feature interpretability but also effectively addresses multicollinearity issues, enhancing the model's robustness and generalization capabilities when handling highly correlated features.\n",
    "- Representation Learning : Guides the latent space with label information, enabling dimensionality reduction and producing discriminative and interpretable embeddings beneficial for downstream classification or regression tasks. Additionally, SUAVE integrates multi-task learning, allowing the incorporation of information from various downstream prediction tasks into the latent space learning process by adjusting task weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = data_preprocess(full_data.copy(), symbol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for SuaveClassifier\n",
    "# transforming the target variable into a 3 bins classification problem\n",
    "Y = pd.concat([y, pd.qcut(y, 3, labels=False)], axis=1, keys=['responder_6', 'target'])\n",
    "target = pd.DataFrame(Y['target'])\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, target, test_size=0.2, shuffle=False)\n",
    "# convert the data into dataframes\n",
    "Y_train = pd.DataFrame(Y_train, columns=['target'])\n",
    "Y_test = pd.DataFrame(Y_test, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Instantiate the model\n",
    "suave_model = SuaveClassifier(input_dim=X_train.shape[1],             # Input feature dimension\n",
    "                        task_classes=[len(Y_train['target'].unique())],   # Number of binary classification tasks\n",
    "                        latent_dim=X_train.shape[1]//2                 # Latent dimension\n",
    "                        )\n",
    "\n",
    "# Fit the model on training data\n",
    "suave_model.fit(X_train, Y_train, epochs=100, animate_monitor=True, verbose=1)\n",
    "\n",
    "# save the model\n",
    "joblib.dump(suave_model, \"suave-model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the latent representation of the data\n",
    "X_train_latent_SUAVE = vae_model.transform(X_train.values)\n",
    "X_test_latent_SUAVE = vae_model.transform(X_test.values)\n",
    "\n",
    "# Fit a linear regression model on the latent representation\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_latent_SUAVE, Y_train)\n",
    "\n",
    "# Predict the target variable on the test set\n",
    "y_pred = linear_model.predict(X_test_latent_SUAVE)\n",
    "_,_,_,y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "\n",
    "# Plot the predictions vs actual values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "plt.plot(y_pred, label='Predicted')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Responder 6')\n",
    "plt.title('Predictions vs Actual Values for symbol 0 with reduced dimensionality using SUAVE VAE and Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
